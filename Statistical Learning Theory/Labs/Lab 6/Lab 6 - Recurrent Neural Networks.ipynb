{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Neural Networks with Keras:\n",
    "\n",
    "In this lab, we will construct different kinds of recurrent neural networks with Keras and show how they many be used for classification, prediction and a little bit of fun. \n",
    "\n",
    "Recall that a recurrent neural network is any network with nodes that update their state between *prediction* runs. This is in contrast to a perceptron or CNN which only updates it state during training. A RNN is composted of __memory cells__, which hold a state between prediction runs, the most popular of which are LSTM and GRU cells. RNN's are trained by \"unfolding\" them to the desired input size, with the weights of each cell shared across the unfolding:\n",
    "\n",
    "<img width=800px src=\"https://raw.githubusercontent.com/tipthederiver/Math-7243-2020/master/Labs/Lab%206/Lab%206%20RNN%20Unroll.PNG\">\n",
    "\n",
    "There are four standard architectures for RNNs: \n",
    "\n",
    "<table><tr>\n",
    "    <td><font size=\"4\"><center>Many to Many</center></font></td><td><font size=\"4\"><center>Many to Few</center></font></td>\n",
    "    </tr><tr>\n",
    "    <td style=\"padding-right: 5em;\"><img width=300px src=\"https://raw.githubusercontent.com/tipthederiver/Math-7243-2020/master/Labs/Lab%206/Lab%206%20RNN%20Type1.PNG\"></td>\n",
    "    <td><img width=300px src=\"https://raw.githubusercontent.com/tipthederiver/Math-7243-2020/master/Labs/Lab%206/Lab%206%20RNN%20Type2.PNG\"></td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "    <td><font size=\"4\"><center>Few to Many</center></font></td><td><font size=\"4\"><center>Variable to Variable</center></font></td>\n",
    "    </tr><tr>\n",
    "    <td style=\"padding-right: 5em;\"><img width=300px src=\"https://raw.githubusercontent.com/tipthederiver/Math-7243-2020/master/Labs/Lab%206/Lab%206%20RNN%20Type3.PNG\"></td>\n",
    "    <td><img width=300px src=\"https://raw.githubusercontent.com/tipthederiver/Math-7243-2020/master/Labs/Lab%206/Lab%206%20RNN%20Type4.PNG\"></td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "This lab has three parts:\n",
    "\n",
    "* Construct a __many to few__ RNN for the classification of text sentiment. We will be classifying the IMDB user comments database to see to see if reviews are positive or negative. \n",
    "* Use a __many to few/many__ RNN to forecast pollution levels in Beijing based on past weather data. \n",
    "* Finally, construct a few to many text generation engine using an RNN. \n",
    "\n",
    "This lab is based on Hands On Machine Learning With Scikit Learn, Tensorflow and Keras by Geron, and [Jason Brownlee's excellent tutorial on text generation with LSTM's](https://machinelearningmastery.com/text-generation-lstm-recurrent-neural-networks-python-keras/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IMDB User Sentiment and Many to One RNN's\n",
    "\n",
    "First, we're going to build a many-to-one RNN, that is we take in many inputs but only record a single output. The dataset will be the IMDB review data set and we will be categorizing reviews as positively or negatively valanced. \n",
    "\n",
    "#### Understanding the Data\n",
    "\n",
    "Before proceeding, we have to preprocess the data. The IMDB dataset has 50000 movies reviews from the Internet Movie Database, 25000 for training and 25000 for testing. Half of the reviews are positive (1) and half are negative (0). The words have already been processed into an integer index, with each word being represented by an integer. The translation diction is given by \n",
    "\n",
    "`word_to_id = imdb.get_word_index()`\n",
    "\n",
    "which is a dictionary of `(word, int)` key-value pairs. Unfortunately this word list doesn't include __padding character__ `0`, the __start character__ `1` and the __unknown word character__ `2` so we have to shift the dictionary and manually add them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import imdb\n",
    "\n",
    "NB_WORDS = 10000\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=NB_WORDS)\n",
    "\n",
    "word_to_id = imdb.get_word_index()\n",
    "word_to_id = {k:(v+3) for k,v in word_to_id.items()}\n",
    "word_to_id[\"<PAD>\"] = 0\n",
    "word_to_id[\"<START>\"] = 1\n",
    "word_to_id[\"<UNK>\"] = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To form the id to word dictionary, we loop over the __(key, value)__ pairs and for a new dictionary of __(value,key)__ pairs using\n",
    "\n",
    "`reverse_word_index = dict([(value, key) for (key, value) in word_to_id.items()])`\n",
    "\n",
    "We can then decode a review, say `x_train[0]` by looping through the elements of `x_train[0]` and looking them up with `reverse_word_index`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reverse_word_index = dict([(value, key) for (key, value) in word_to_id.items()])\n",
    "\n",
    "review = [reverse_word_index[word] for word in x_train[0]]\n",
    "' '.join(review)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we will not be dynamically unrolling the RNN for training, but instead fix the maximum review length at 80 and pad any review with fewer than 80 characters. The `keras.preprocessing` library has a `sequence` class with utilities for dealing with sequences ([see the documentation for more information](https://keras.io/preprocessing/sequence/)). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing import sequence\n",
    "\n",
    "MAXLEN = 80\n",
    "    \n",
    "X_train = sequence.pad_sequences(x_train, maxlen=MAXLEN)\n",
    "X_test = sequence.pad_sequences(x_test, maxlen=MAXLEN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building the RNN\n",
    "Now that we understand our data, lets build our RNN. To construct the RNN, we will have an embedding layer, densely connected to a layer of 32 LSTMs, densely connected to a single output node. Since this is a binary classification problem, we will use `binary_crossentropy`\n",
    "\n",
    "\n",
    "* __The Embedding Layer:__ The embedding layer is itself a little neural network that we train to attempt to embed the high dimensional one-hot encoding of the text into a lower dimensional representation for ease of processing. Word2Vec is a two layer network that interpolates between the number of words `NB_WORDS` and the desired embedding dimension `EMBED_DIM`.  GloVec is an example of an unsupervised network that learns via taking the \"context distance\" between words in the training set and forming a Euclidean space. See [Jason Brownlees blog](https://machinelearningmastery.com/use-word-embedding-layers-deep-learning-keras/) for more about word embeddings.\n",
    "\n",
    "In the architecture below, Keras understands that since the dense layer is not a reccurence cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import Sequential\n",
    "from keras.layers import Dense, Embedding, Dropout, LSTM\n",
    "# model parameters:\n",
    "EMBED_DIM = 50\n",
    "LSTM_UNITS = 32\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Embedding(NB_WORDS, EMBED_DIM, input_length=MAXLEN))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(LSTM_UNITS))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "\n",
    "# try using different optimizers and different optimizer configs\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the vast majority of the variables for the network above come from the embedding layer.\n",
    "\n",
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "validation_split = 0.2\n",
    "\n",
    "history = model.fit(X_train, y_train, batch_size=128,\n",
    "          epochs=epochs, \n",
    "          validation_split=validation_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(5,3))\n",
    "plt.plot(history.epoch,history.history['loss'], label='training')\n",
    "plt.plot(history.epoch,history.history['val_loss'], label='validation')\n",
    "plt.title('loss')\n",
    "plt.legend(loc='best')\n",
    "\n",
    "plt.figure(figsize=(5,3))\n",
    "plt.plot(history.epoch,history.history['acc'], label='training')\n",
    "plt.plot(history.epoch,history.history['val_acc'], label='validation')\n",
    "plt.title('accuracy')\n",
    "plt.legend(loc='best');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise:\n",
    "\n",
    "LSTMs often have problems with over fitting. Try adding another dropout layer between the LSTM and the output layer and compare with the results above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise:\n",
    "\n",
    "Change the network above to be deeper, but thinner by adding another RNN layer both with 16 nodes. How do the loss accuracy, and training time compare to the first network?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Many to Many RNN's: Forecasting Beijing's Polution Levels. \n",
    "\n",
    "Lets now take a look at time series data using a many to many RNN. We will follow closely the tutorial here:\n",
    "\n",
    "https://machinelearningmastery.com/multivariate-time-series-forecasting-lstms-keras/\n",
    "\n",
    "We will be looking at the Beijing Air Quality dataset. The data set consists of labels\n",
    "\n",
    "\n",
    "|Label|Description|Label|Description|\n",
    "|-----|-----------|-----|-----------|\n",
    "|No| row number|DEWP| Dew Point|\n",
    "|year| year of data in this row|TEMP| Temperature|\n",
    "|month| month of data in this row|PRES| Pressure|\n",
    "|day| day of data in this row|cbwd| Combined wind direction|\n",
    "|hour| hour of data in this row|Iws| Cumulated wind speed|\n",
    "|pm2.5| PM2.5 concentration|Is| Cumulated hours of snow|\n",
    "|Ir| Cumulated hours of rain|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv(\"https://raw.githubusercontent.com/jbrownlee/Datasets/master/pollution.csv\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pandas can actually parse the date as it comes in by setting `parse_dates` to the fields containing dates and `date_parser`  to a function. Lets read the file in again parsing the year, month, day and hour using `datatime`. We'll also drop the `No` tag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "def parse(x):\n",
    "    return datetime.strptime(x, '%Y %m %d %H')\n",
    "\n",
    "data = pd.read_csv(\"https://raw.githubusercontent.com/jbrownlee/Datasets/master/pollution.csv\",  \n",
    "                   parse_dates = [['year', 'month', 'day', 'hour']], \n",
    "                   index_col=0, \n",
    "                   date_parser=parse)\n",
    "data.drop('No', axis=1, inplace=True)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We drop the `NaN`s in `pm2.5` by filling them with zeros using `.fillna(0, inplace=True)`. Then, we rename the column for readability. The data is now an index so we must set it with a different call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['pm2.5'].fillna(0, inplace=True)\n",
    "\n",
    "data.columns = ['pollution', 'dew', 'temp', 'press', 'wnd_dir', 'wnd_spd', 'snow', 'rain']\n",
    "data.index.name = 'date'\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets take a look at our data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, axes = plt.subplots(4,2,figsize=(15,10))\n",
    "plt.tight_layout()\n",
    "\n",
    "axes = axes.reshape(-1)\n",
    "\n",
    "for i in range(8):\n",
    "    axes[i].plot(data.values[:,i])\n",
    "    axes[i].set_title(data.columns[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training Data Preparation\n",
    "\n",
    "We now need to prepare training data for the RNN to fit to. The data we construct will depend on the problem we are trying to solve. In this demonstration, we will formulate a network that solves the conditions for the pollution at the current hour, given the measurements at the previous time step. Some alternate formulations you could explore include:\n",
    "\n",
    "* Predict the pollution for the next hour based on the weather conditions and pollution over the last 24 hours.\n",
    "* Predict the pollution for the next hour as above and given the \"expected\" weather conditions for the next hour."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop(\"wnd_dir\", axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We scale the data so that all of the ranges are between 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "values = data.values\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "scaled = scaler.fit_transform(values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will form sequences of data points, representing the five previous hours, and try to predict the weather the next hour. The easiest we to do this is to construct a new matrix by shifting the training data by one unit and concatanating the shifted matrices together along a new dimension:\n",
    "\n",
    "<img width = 500 src=\"https://raw.githubusercontent.com/tipthederiver/Math-7243-2020/master/Labs/Lab%206/Lab%206%20Shiftdata.png\">\n",
    "\n",
    "This way of producing a training set of sequences from raw data is much faster than building the sequences using a loop. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "SEQ_LEN = 5\n",
    "DATA_LEN = scaled.shape[0]\n",
    "\n",
    "X_train = scaled[0:-SEQ_LEN-1,:].reshape(-1,1,7)\n",
    "\n",
    "for i in range(1,SEQ_LEN):\n",
    "    X_train = np.append(X_train, scaled[i:-SEQ_LEN+i-1,:].reshape(-1,1,7), axis=1)\n",
    "    \n",
    "Y_train = scaled[SEQ_LEN:-1,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets check that we've constructed out sequences correctly. For example, sequence 40 should contain data points 40 to 44 and the corresponding y should be the pollution from datapoint 45:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train[40,:,0])\n",
    "print(Y_train[40])\n",
    "print(scaled[40:46,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape)\n",
    "print(Y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, lets built a model ad before. It will have an LSTM layer with 50 nodes that attach densely to a single output node. For loss we use __mean squared error__, but you can also try using __mean absolute error__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(50, input_shape=(X_train.shape[1], X_train.shape[2])))\n",
    "model.add(Dense(1))\n",
    "model.compile(loss='mse', optimizer='adam')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we run the model and plot the training and validation loss as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "epochs = 100\n",
    "validation_split = 0.2\n",
    "\n",
    "history = model.fit(X_train, Y_train, batch_size=128,\n",
    "          epochs=epochs, \n",
    "          validation_split=validation_split)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(5,3))\n",
    "plt.plot(history.epoch,history.history['loss'], label='training')\n",
    "plt.plot(history.epoch,history.history['val_loss'], label='validation')\n",
    "plt.title('loss')\n",
    "plt.legend(loc='best')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let see how we did visually. We'll use the model to predict the next hour pollution levels on 40 or so data points and compare it to the actual next hour pollution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.array([])\n",
    "OFFSET = 100\n",
    "LEN = 40\n",
    "for i in range(0,LEN):\n",
    "    A = np.append(A,model.predict(X_train[OFFSET+i].reshape(1,5,7)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(Y_train[OFFSET:OFFSET+LEN],label=\"Measured\")\n",
    "plt.plot(A,label=\"Predicted\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: \n",
    "\n",
    "Use the RNN to generate data: Starting with a training point, have the RNN predict the next hour pollution and then form a new test point by appending the prediction to the test point and dropping the last data point. Repeat this process until you've generated 200 hours of new data. \n",
    "\n",
    "Your generated data will not match up with the true pollution measurements unless you have massively over fit, but does it look reasonable?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Longer forcasting\n",
    "\n",
    "Our results above aren't too shabby, that is until you zoom in farther. You may then find that the graph looks surprisingly  like the RNN has just learned to the current pollution level, albeit shifted slightly. Can we extend this forecast longer?\n",
    "\n",
    "The only change we will need to make to our network is to output more nodes, specifically the number of hours ahead we want to predict. Create a new network, and new data, the predicts the next 5 hours. \n",
    "\n",
    "For more information on time series data generation we here: https://machinelearningmastery.com/time-series-prediction-lstm-recurrent-neural-networks-python-keras/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \"Shall I generate thee a summers soliloquy?\" \n",
    "## Text Generation with few to many RNNs\n",
    "\n",
    "As a final step in RNN learning, lets build a text generation engine. A text generation engine is a few to many RNN, where we prime the network with a small snipit of text and ask it to generate more. For this engine, you can follow along with the copy of the complete works of Shakespeare, which can be found on the course blackboard, any book you have as a text file (Project Gutenburg has many https://www.gutenberg.org/) or any other large volume of text you have access to. \n",
    "\n",
    "This section closely follows https://machinelearningmastery.com/text-generation-lstm-recurrent-neural-networks-python-keras/ where he performs a similar analysis on Alice in Wonderland. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_text = open(\"Shakespeare.txt\", encoding=\"utf8\").read()\n",
    "raw_text = raw_text.lower()\n",
    "\n",
    "display(raw_text[0:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that there are unfortunently line break throughout the document for formatting reasons. We could eliminate the line breaks keeping the paragraph, but for this RNN we only care about sequences of words. Lets replace all `\\n` characters with spaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed = raw_text.replace('\\n',' ')\n",
    "display(processed[0:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will start with a simple character level RNN. We can create a character map using the function `set()`:\n",
    "\n",
    "* `set()` Converts a list in a set object. A set only contains one copy of each instance of the list. \n",
    "\n",
    "By converting the text to a set and then returning it to a list we get a list of all the characters in the novel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chars = sorted(list(set(processed)))\n",
    "print(\"Number of Characters:\", len(chars))\n",
    "print(\"Character Map:\", chars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We might want to process out the bit encoder mark `\\ufeff`, as well as the odder special characters. In fact, for this first pass lets remove all nonstandard punctuation and numbers except tabs, brackets and quotes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_chars = ['&','\\\\','|', '}', '—', '‘', '’', '“', '”','0', '1', '2', '3', '4', '5', '6', '7', '8', '9','à', 'æ', 'è', 'œ', '\\ufeff']\n",
    "data = processed\n",
    "for b in bad_chars:\n",
    "    data = data.replace(b,'')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chars = sorted(list(set(data)))\n",
    "print(\"Number of Characters:\", len(chars))\n",
    "print(\"Character Map:\", chars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we generate a dataset of strings of a desired length, say 60 characters. We will also set a step size so that we take a subset of all possible strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "STEP = 30\n",
    "SEQ_LEN = 60\n",
    "\n",
    "N_CHARS = len(data)\n",
    "\n",
    "X_strings = []\n",
    "y_next_chars = []\n",
    "\n",
    "for i in range(0,N_CHARS - SEQ_LEN, STEP):\n",
    "    X_strings.append(data[i:i+SEQ_LEN])\n",
    "    y_next_chars.append(data[i+SEQ_LEN])\n",
    "    \n",
    "print(\"Number of sequencces: \", len(X_strings))\n",
    "print(X_strings[0:5])\n",
    "print(y_next_chars[0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we one hot encode the vector using `enumerate`. The `index, value = enumerate(list)` function iterates through a list, returning both the index of each element and the value. We generate a matrix of 0's of the appropriate size and loop through put a 1 in the spot associated to each letter. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "N_VOCAB = len(chars)\n",
    "\n",
    "char_indices = dict((c, i) for i, c in enumerate(chars))\n",
    "indices_char = dict((i, c) for i, c in enumerate(chars))\n",
    "\n",
    "x = np.zeros((len(X_strings),SEQ_LEN,N_VOCAB),dtype=np.bool)\n",
    "y = np.zeros((len(y_next_chars),N_VOCAB),dtype=np.bool)\n",
    "\n",
    "for i, string in enumerate(X_strings):\n",
    "    for t, char in enumerate(string):\n",
    "        x[i,t, char_indices[char]] = 1\n",
    "    y[i, char_indices[y_next_chars[i]]] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will be making a sequential model with a a single LSTM layer connected to a dense layer with one node for each character. We then softmax the output to generate a probability vector of what character will come next. This will allow us to stochastically generate the next letter, adding a bit of diversity to the outcome. The last layer will use an Adam optimizer with categorical crossentropy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.layers import LSTM\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import LambdaCallback, ModelCheckpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will put 128 LSTM nodes in our model and attach them densely to a layer of perceptrons with one node for each character. We then softmax to normalize the output to a probability for each each letter. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(124, input_shape=(SEQ_LEN,N_VOCAB)))\n",
    "model.add(Dense(len(chars)))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "optimizer = Adam()\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can optionally save out a model checkpoint if we want to store our fit weights until later (this may be wise for an RNN, it takes longer to train them than more linear networks). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import io\n",
    "\n",
    "# define the checkpoint\n",
    "filepath = \"weights.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, \n",
    "                             monitor='loss', \n",
    "                             verbose=1, \n",
    "                             save_best_only=True, \n",
    "                             mode='min')\n",
    "\n",
    "# fit model using our gpu\n",
    "\n",
    "model.fit(x, y,\n",
    "          batch_size=64,\n",
    "          epochs=5,\n",
    "          verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predicting on a sequence returns a vector of probabilities for each character. We want to define a function to sample the probability vector that we can tune a little bit to be more and less \"creative.\" One way to do this is to scale the probabilities using\n",
    "\n",
    "$$\n",
    "\\tilde{p}_i = \\frac{\\exp\\left( \\frac{1}{t}\\log p_i \\right)}{\\sum_i e^{\\frac1t\\log p_i}} = \\frac{p_i^{\\frac{1}{t}}}{\\sum_i p_i^{\\frac{1}{t}}}\\,.\n",
    "$$\n",
    "\n",
    "Increasing $t$ normalizes all the probabilities, bringing them closer together. We then use `np.random.multinomial` to randomly sample the probabilities, returning a 1 in the chosen index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(preds, temperature=1.0):\n",
    "    # helper function to sample an index from a probability array\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "sentence = data[50: 50 + SEQ_LEN]\n",
    "diversity = .5\n",
    "generated = ''\n",
    "generated += sentence\n",
    "\n",
    "for i in range(400):\n",
    "    x_pred = np.zeros((1, SEQ_LEN, len(chars)))\n",
    "    for t, char in enumerate(sentence):\n",
    "        x_pred[0, t, char_indices[char]] = 1.\n",
    "\n",
    "    preds = model.predict(x_pred, verbose=0)[0]\n",
    "    next_index = sample(preds, diversity)\n",
    "    next_char = indices_char[next_index]\n",
    "\n",
    "    generated += next_char\n",
    "    sentence = sentence[1:] + next_char\n",
    "\n",
    "    sys.stdout.write(next_char)\n",
    "    sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: :\n",
    "\n",
    "Repeat the code above using a GRU cell as opposed to a LSTM cell. Compare the training time and accuracy of the two networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: :\n",
    "\n",
    "There is nothing in principle different from the character level RNN above and a word level RNN. Make a RNN to generate sentences from words instead of characters (you may clean out all punctuation to make this easier). It may help to only use the words that appear more than one, more than twice or more than three times, dropping any phrases that include unique words. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problems: (Geron, 15.10)\n",
    "\n",
    "<img width = 800 src = http://www.bach-chorales.com/Images/ChoraleImages/Image_BWV_0133_6.jpg>\n",
    "\n",
    "Download the [Bach chorales dataset](https://homl.info/bach) and unzip it. It is composed of 382 chorales composed by Johann Sebastian Bach. Each chorale is 100 to 640 time steps long, and each time step contains 4 integers, where each integer corresponds to a note’s index on a piano (except for the value 0, which means that no note is played).\n",
    "\n",
    "Train a model — recurrent, convolutional, or both — that can predict the next time step (four notes), given a sequence of time steps from a chorale. Then use this model to generate Bach-like music, one note at a time: you can do this by giving the model the start of a chorale and asking it to predict the next time step, then appending these time steps to the input sequence and asking the model for the next note, and so on. Also make sure to check out [Google’s Coconet model](https://magenta.tensorflow.org/coconet), which was used for a nice Google doodle about Bach.\n",
    "\n",
    "#### Further information\n",
    "\n",
    "At each time step, each chorales has 4 notes. Looking at the CSV for one of  the files we see that the columns are __note0__, __note1__, __note2__ and __note3__, with each row corresponding to a timestep. Each of the numbers corresponds to a piano key. \n",
    "\n",
    "Python can construct audio and Jupyter can play it in a Jupyter widget:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Audio\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "signal = np.random.random(750)\n",
    "Audio(signal, rate=250)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The audio is then constructed by a simple sum of sine functions. I've uploaded in index between the note position and the keys taken from Wikipeidia (https://en.wikipedia.org/wiki/Piano_key_frequencies)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "piano_path = \"https://raw.githubusercontent.com/tipthederiver/Math-7243-2020/master/Labs/Lab%206/Piano%20Notes.csv\"\n",
    "notes = pd.read_csv(piano_path, encoding= 'unicode_escape')\n",
    "notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then get the frequency from the key position by"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "notes[notes[\"Key Position\"]==1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function below will take a sequence of notes and turn it into audio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_audio(song, notes, framerate=22050, L=.25):\n",
    "    framerate = 22050  ## Standard Framerate\n",
    "    N = len(song) # Number of Notes\n",
    "    L = .25        # Note Length in Seconds\n",
    "    W = int(framerate*L) # Window Size\n",
    "    t = np.linspace(0,L,W)\n",
    "    data = np.zeros(W*N)\n",
    "\n",
    "    for i in range(N): \n",
    "        F = notes[\"Frequency (Hz)\"].iloc[song[i]+1]\n",
    "        data[W*i:W*(i+1)] = np.sin(2*np.pi*F*t)\n",
    "        \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " I've included two note snippets below. These are __note0__ and __note1__ from chorale 305:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "song_0 = [65,65,65,65,72,72,70,70,69,69,67,67,65,65,65,65,72,72,72,72,74,74,74,74,74,74,74]\n",
    "song_1 = [60,60,60,60,60,60,60,60,60,60,60,60,62,62,64,64,65,65,65,65,65,65,65,65,65,65,65]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can generate the tunes individually:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_0 = gen_audio(song_0, notes)\n",
    "Audio(data,rate=22050)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "or to hear them together, simply add the sine-wave representation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_0 = gen_audio(song_0, notes)\n",
    "data_1 = gen_audio(song_1, notes)\n",
    "Audio(data_0 + data_1,rate=22050)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rough Outline: \n",
    "\n",
    "This entire project can be completed without ever listening to the audio files and instead just treating the sequences as sequences. Your pattern should roughly be the text generator above, although now at each time step we have 4 notes, not a single letter. A rough outline is as follows:\n",
    "\n",
    "* Load a single chorale using Panda's `read_csv` function.\n",
    "* Construct the training data as we did for the text generator: the `X_train` will be sequences of $K$ timesteps, each time step containing the 4 notes. It is your choice if you leave them as integers of one-hot encode the notes. \n",
    "* The labels `y_train` will be sequences of length $K$ timesteps shifted by 1.\n",
    "* Construct a simple RNN model with 64 LSTM nodes. Your input and output shape will be $K\\times 4$ if you do not one-hot encode and $K\\times 4\\times 108$ if you do one-hot encode.\n",
    "* After your get your network running on one chorale, expand your dataset by adding sequences from other chorales. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
