{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to TensorFlow and Keras\n",
    "\n",
    "TensorFlow is Google's machine learning programing language. TensorFlow is a very general tools for constructing artificial neural networks, both as operation trees and as more topologically complicated networks. TensorFlow's nodes incorporate all of the structure we discussed in the lecture, including most importantly the reverse autodiff partial derivative information.\n",
    "\n",
    "It's important to remember that using TensorFlow for computations proceeds in two parts:\n",
    "\n",
    "1. __Construct a graph:__ In this step, create a pattern for the network by defining all of our nodes (input, output, constant, multipliers, LSUs, etc) and connecting them. You also specify their initial values. At this stage no construction has been done. \n",
    "\n",
    "2. __Instantiate the model:__ Start a tensor flow __session__ and instantiate the model. An instantiation of the neural network actually creates a copy of all the variables in memory and stores their initial values in them. I\n",
    "\n",
    "3. __Running the graph:__  Within the TensorFlow __session__, begin running/training the network. \n",
    "\n",
    "<img src = \"https://www.tensorflow.org/images/tensors_flowing.gif\">\n",
    "\n",
    "Higher level tools like Keras hide the details of the initialization and session and streamline this process by including pre-made versions of common architectures. \n",
    "\n",
    "This lab will consist of three parts: \n",
    "\n",
    "* __Part 1__: First, we will show how to create a simple computational tree using TensorFlow and how to run it using a session. \n",
    "* __Part 2__: Second, we will create a single layer perceptron with TensorFlow and use it to classify the binary labeling.\n",
    "* __Part 3__: Finally, we will create a multilayer perceptron with TensorFlow and use it to classify the binary labeling.\n",
    "\n",
    "#### Getting TensorFlow and Keras\n",
    "\n",
    "If you are on Google Colab, TensorFlow and Keras are automatically installed. If you are running Jupyter locally use the anaconda prompt to install using \n",
    "\n",
    "    $ pip install tensorflow\n",
    "$ pip install keras\n",
    "\n",
    "* Windows: Open \"Anaconda Prompt\" from the start menu or from Cortana's search. \n",
    "* OSX: Open terminal, you should directly be able to use pip from there. \n",
    "* Linux: Open terminal, you should directly be able to use pip from there. \n",
    "\n",
    "For more information, see https://www.tensorflow.org/install and https://keras.io/#installation.\n",
    "\n",
    "This lab follows Chapters 9 and 10 from *Hands-On Machine Learning with Scikit_Learn & TensorFlow*.\n",
    "\n",
    "#### This notebook is based heavily on Geron's companion notebook\n",
    "\n",
    "**Chapter 10 â€“ Introduction to Artificial Neural Networks with Keras**\n",
    "\n",
    "This notebook can be found at \n",
    "\n",
    "https://github.com/ageron/handson-ml2\n",
    "\n",
    "#### Lab 4.5: Tensorflow\n",
    "\n",
    "This lab skips over Tensorflow and moves directly to using Keras to construct artificial neural networks. For an introduction to Tensorflow see Lab 4.5, which is not required. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's import a few common modules, ensure MatplotLib plots figures inline and pretty them up a bit. Now that we've had some time to play around with pyplot, we will use matplotlib directly to set the font and label sizes globally. For more information about customization with rc see\n",
    "\n",
    "https://matplotlib.org/3.1.3/tutorials/introductory/customizing.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# to make this notebook's output stable across runs\n",
    "np.random.seed(42)\n",
    "\n",
    "# To plot pretty figures\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "mpl.rc('axes', labelsize=14)\n",
    "mpl.rc('xtick', labelsize=18)\n",
    "mpl.rc('ytick', labelsize=18)\n",
    "mpl.rc('legend', fontsize=18)\n",
    "\n",
    "\n",
    "# Ignore useless warnings (see SciPy issue #5998)\n",
    "import warnings\n",
    "warnings.filterwarnings(action=\"ignore\", message=\"^internal gelsd\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will start by fitting a perceptron to some of the features of the UW Breast Cancer dataset. Loading the data below, we set our training data to the features we selected in Lab 3 as probably indicative of the labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/wdbc.data')\n",
    "names = [\"id\",\"diagnosis\",\"radius_mean\",\"texture_mean\",\"perimeter_mean\",\"area_mean\",\n",
    "         \"smoothness_mean\",\"compactness_mean\",\"concavity_mean\",\"concave_points_mean\",\n",
    "         \"symmetry_mean\",\"fractal_dimension_mean\",\"radius_se\",\"texture_se\",\"perimeter_se\",\n",
    "         \"area_se\",\"smoothness_se\",\"compactness_se\",\"concavity_se\",\"concave\" \"points_se\",\n",
    "         \"symmetry_se\",\"fractal_dimension_se\",\"radius_worst\",\"texture_worst\",\n",
    "         \"perimeter_worst\",\"area_worst\",\"smoothness_worst\",\"compactness_worst\",\n",
    "         \"concavity_worst\",\"concave_points_worst\",\"symmetry_worst\",\"fractal_dimension_worst\"]\n",
    "\n",
    "data.columns=names\n",
    "\n",
    "\n",
    "## Drop Feature Columns X\n",
    "X = data.drop(columns=[\"id\",\"diagnosis\"])\n",
    "\n",
    "## Set Up Target Variables y\n",
    "y = data[\"diagnosis\"]\n",
    "\n",
    "## Normalize feature data by centering on the mean and dividing by std\n",
    "X = (X - X.mean())/X.std()\n",
    "\n",
    "f, ax = plt.subplots(figsize=(8,8))\n",
    "\n",
    "I_m = y==\"M\"\n",
    "I_b = y==\"B\"\n",
    "\n",
    "plt.plot(X[\"radius_mean\"][I_m],X[\"concavity_mean\"][I_m],'o',label=\"Malignant\")\n",
    "\n",
    "## We set alpha=.5 to try to avoid masking, but some points still will be burried. \n",
    "plt.plot(X[\"radius_mean\"][I_b],X[\"concavity_mean\"][I_b],'o',label=\"Benign\",alpha=.5)\n",
    "\n",
    "plt.xlabel(\"radius_mean\")\n",
    "plt.ylabel(\"concavity_mean\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we need to import Tensorflow and Keras. Within Keras, a perceprton is just what is called a __dense layer__, this is because the multicategory perceptron has a single weight for each connection between the input later and the output layer:\n",
    "\n",
    "<img width = 50% src=\"https://github.com/tipthederiver/Math-7243-2020/raw/master/Labs/Lab%204/Lab4MultilabPerceptron.PNG\">\n",
    "\n",
    "To code this in keras, we need to specify five things:\n",
    "\n",
    "* The dimension of the input.\n",
    "* The dimension of the output.\n",
    "* The activation function for the LTUs.\n",
    "* The loss function. \n",
    "* The training method, or the optimizer.\n",
    "\n",
    "This kind of model is a __sequential model__ since the data starts at one end and passes through to the other end which each layer acting on all of the data. As such, we start by creating a sequential model using `keras.models.Sequential()` and then adding a `Dense` layer as below. Since the dense layer is an input layer, we will need to specify the input size the dense lay should expect, in this case 2:\n",
    "\n",
    "* `keras.layers.Dense(2,[input_dim=2,] activation=\"relu\")` Create a layer densely connecting to all nodes in the layer before it. If this is the input layer, specify the dimensionality of the input. \n",
    "\n",
    "In this case we will have two input dimensions (__radius_mean__ and __concavity_mean__) and 2 output dimensions (__M__ and __B__)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "# Define Sequential Model\n",
    "# Add a dense layer connecting the 2 inputs to 2 outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see a summary of the model we've built by using `model.summary`, include the number of parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summarize the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These six parameters are the four weights in the $2\\times 2$ weight matrix, plus the two bias $B$ parameters:\n",
    "\n",
    "$$\n",
    "\\hat{y} = \\sigma(WX + B)\\,,\n",
    "$$\n",
    "\n",
    "where $\\sigma$ here is the ReLU function acting on each coordinate separately. To compile the model, we run `model.compile()`. We need to specify the __loss__, and the __optimizer__. We can also optionally specify a __metric__ that will be reported so that we can evaluate the model, but will not be used in training.\n",
    "\n",
    "* `model.compile(loss=, optimizer= [,metric=[] ])` - Compile the model with the given loss function, optimize and metrics.\n",
    "\n",
    "In this case we will use the mean squared error for the loss `mean_squared_error`, stochastic gradient decent for the optimizer `sgd` and accuracy for the metric `accuarcy`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Compile the model specifying loss = mean_squared_error, optimizer = sgd and metric = accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we fit the model. We will have sklearn perform the train set split for us:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X[[\"radius_mean\",\"concavity_mean\"]], \n",
    "                                                    pd.get_dummies(y), \n",
    "                                                    test_size=0.2, \n",
    "                                                    random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "history = model.fit(X_train, y_train, epochs=20,\n",
    "                    validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10,10))\n",
    "\n",
    "N = 50\n",
    "\n",
    "## Create the meshgrid using the range of the input data features\n",
    "## Reshape the meshgrid to a vector and predict labels using our model\n",
    "## Plot it. \n",
    "\n",
    "plt.plot(XX[ZZ==0],YY[ZZ==0],'.')\n",
    "plt.plot(XX[ZZ==1],YY[ZZ==1],'.')\n",
    "plt.scatter(X_train.radius_mean,X_train.concavity_mean,c=np.argmax(np.array(y_train),axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It may take a few more epochs then expected, but the perceptron does eventually find a good classifier. \n",
    "\n",
    "#### Exercise:\n",
    "\n",
    "In the network above, we had to choose the loss function, the activation function for the LTU and the optimizer. These are not the only choices we could have made. For example, for activation function we have chosen\n",
    "\n",
    "Loss Functions (https://keras.io/losses/)\n",
    "* `mean_squared_error`: The standard RSS error for numerical data $L(y,\\hat{y})= \\frac{1}{N}\\sum_{i=1}^N (y-\\hat{y})^2$.\n",
    "* `mean_absolute_error`: The absolute error for numerical data $L(y,\\hat{y}) = \\frac{1}{N}\\sum_{i=1}^N |y-\\hat{y}|$.\n",
    "* `categorical_crossentropy:`The categorical cross entropy\n",
    "$$\n",
    "L(y,\\hat{y}) = -\\sum_{i=1}^N \\sum_{k=1}^K y_{ik}\\,\\log(\\hat{y}_{ik})\\,.\n",
    "$$\n",
    "    The categorical cross entropy assume the values are probabilities, and so take values between 0 and 1. \n",
    "    \n",
    "Activation Functions (https://keras.io/activations/)\n",
    "\n",
    "* `relu`: The ReLU function, used for regression problems. \n",
    "$$(x)_+ = \\begin{cases}0&x<0\\,,\\\\x&x\\geq0\\,.\\end{cases}$$\n",
    "* `tanh`: The hyperbolic tangent function with values between -1 and 1. Used for regression. \n",
    "* `sigmoid`: The binary sigmoid activation with values between 0 and 1. Used for binary classification $\\sigma(x) = \\frac{1}{1 + e^{-x}}$.\n",
    "* `softmax`: The softmax function converts a vector of values into approximate probability for each element of the vectors. Used in multicategory classification: $\\sigma:\\mathbb{R}^K\\to\\mathbb{R}^K$, via\n",
    "$$\n",
    "\\sigma(x)_k = \\frac{e^{-x_k}}{\\sum_{k=1}^K e^{-x_k}}\n",
    "$$\n",
    "\n",
    "Try the functions above in the network. Why would you expect `softmax` to work better with `categorical_crossentropy`?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A first deep network:\n",
    "\n",
    "Deepening our network is as simple as adding another dense layer on, although now we can specify the number of nodes in the internal layer. In the code below, we insert a hidden layer with 10 nodes. We still have an input of 2 and and output of 2, keras works out the connections for the intermediate layers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "model = keras.models.Sequential()\n",
    "\n",
    "## Add two more layers:\n",
    "## One hiddden layer densly connecting the input later to 10 nodes and \n",
    "## One output layer with 2 output layers. \n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise: \n",
    "\n",
    "We now have 52=$2\\times 10 + 10 + 10\\times 2 + 2$ parameters. Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to install pydot, the network can be visualized with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "keras.utils.plot_model(model, \"my_fashion_mnist_model.png\", show_shapes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can now compile, train and visualize the deep network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"mean_squared_error\",\n",
    "              optimizer=\"sgd\",\n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "history = model.fit(X_train, y_train, epochs=20,\n",
    "                    validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10,10))\n",
    "\n",
    "N = 50\n",
    "\n",
    "x_mesh = np.linspace(X_train.radius_mean.min(), X_train.radius_mean.max(), N)\n",
    "y_mesh = np.linspace(X_train.concavity_mean.min(), X_train.concavity_mean.max(), N)\n",
    "[XX,YY] = np.meshgrid(x_mesh,y_mesh)\n",
    "XX = XX.reshape(-1,1)\n",
    "YY = YY.reshape(-1,1)\n",
    "\n",
    "D = np.concatenate([XX,YY],axis=1)\n",
    "ZZ = np.argmax(model.predict(D),axis=1)\n",
    "\n",
    "plt.plot(XX[ZZ==0],YY[ZZ==0],'.')\n",
    "plt.plot(XX[ZZ==1],YY[ZZ==1],'.')\n",
    "plt.scatter(X_train.radius_mean,X_train.concavity_mean,c=np.argmax(np.array(y_train),axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building an Image Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let's import TensorFlow and Keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by loading the fashion MNIST dataset. Keras has a number of functions to load popular datasets in `keras.datasets`. The dataset is already split for you between a training set and a test set, but it can be useful to split the training set further to have a validation set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fashion_mnist = keras.datasets.fashion_mnist\n",
    "(X_train_full, y_train_full), (X_test, y_test) = fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training set contains 60,000 grayscale images, each 28x28 pixels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get the shape of the training file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each pixel intensity is represented as a byte (0 to 255):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get the datatype of the training file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's split the full training set into a validation set and a (smaller) training set. We also scale the pixel intensities down to the 0-1 range and convert them to floats, by dividing by 255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_valid, X_train = X_train_full[:5000] / 255., X_train_full[5000:] / 255.\n",
    "y_valid, y_train = y_train_full[:5000], y_train_full[5000:]\n",
    "X_test = X_test / 255."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can plot an image using Matplotlib's `imshow()` function, with a `'binary'`\n",
    " color map:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(X_train[0], cmap=\"binary\")\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The labels are the class IDs (represented as uint8), from 0 to 9:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the corresponding class names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = [\"T-shirt/top\", \"Trouser\", \"Pullover\", \"Dress\", \"Coat\",\n",
    "               \"Sandal\", \"Shirt\", \"Sneaker\", \"Bag\", \"Ankle boot\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the first image in the training set is a coat:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names[y_train[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The validation set contains 5,000 images, and the test set contains 10,000 images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_valid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at a sample of the images in the dataset. We will also look at a final way of looping through sublplots:\n",
    "\n",
    "* `plt.subplot(n_rows, n_columns, i)` - Gets the current axis, ie the axis `plt.plot` plots to, to `i`'thaxis in the grid of subplots with `n_rows` rows and `n_columns` columns. This allows us to loop through a grid of subplots using the `plt.` syntax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_rows = 4\n",
    "n_cols = 10\n",
    "plt.figure(figsize=(n_cols * 1.2, n_rows * 1.2))\n",
    "for row in range(n_rows):\n",
    "    for col in range(n_cols):\n",
    "        index = n_cols * row + col\n",
    "        plt.subplot(n_rows, n_cols, index + 1)\n",
    "        plt.axis('off')\n",
    "        # Plot the i'th training element\n",
    "        # Title the image with the true label\n",
    "        \n",
    "plt.subplots_adjust(wspace=0.2, hspace=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just as before, we want to construct a model to classify the images. Since each of the images is $28\\times 28$, we have two options: \n",
    "\n",
    "1. We can reshape the images as we have before and use an input layer with 784 inputs.\n",
    "2. We can add a `keras.layers.Flatten` layer to our network with an input shape of `[28,28]` and let is reshape the data for us.\n",
    "\n",
    "We will use the latter method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Flatten(input_shape=[28, 28]))\n",
    "## Add a layer densely connecting the input layer to 300 nodes with activation ReLU\n",
    "## Add a layer densely connecting the dense layer to 100 nodes with activation ReLU\n",
    "## Add a layer densely connecting the second sense layer 10 nodes with activation softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets take a look at our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "keras.utils.plot_model(model, \"my_fashion_mnist_model.png\", show_shapes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`model` itself is an object containing all of network information. For example, if we would like information about the layers we can query them from the `model.layers`. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden1 = model.layers[1]\n",
    "hidden1.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.get_layer(hidden1.name) is hidden1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also dump the weights and biases if we so wish, using each layers `layer.get_weights()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights, biases = hidden1.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "biases.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets compile and fit the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Compile the model with loss: sparse_categorical_crossentropy, optimizer: sgg, and metric: accuracy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Fit the model for 30 epochs, vailidating on the validation data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tracking training:\n",
    "\n",
    "Now that we've trained out model, we can use the `history` object we saved our training model into to track the training. The history has all of the parameters we passed to the trainer in `history.params`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history.params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It also has any metrics we may have been using stored in for each epoch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.DataFrame(history.history).plot(figsize=(8, 5))\n",
    "plt.grid(True)\n",
    "plt.ylim(0, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can evaluate the model using the evaluate function. This will return the any metrics we specified at the time of the function call:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, as before we can use our model to predict the label on new data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new = X_test[:3]\n",
    "y_proba = model.predict(X_new)\n",
    "y_proba.round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict_classes(X_new)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(class_names)[y_pred]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_new = y_test[:3]\n",
    "y_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(7.2, 2.4))\n",
    "for index, image in enumerate(X_new):\n",
    "    plt.subplot(1, 3, index + 1)\n",
    "    plt.imshow(image, cmap=\"binary\", interpolation=\"nearest\")\n",
    "    plt.axis('off')\n",
    "    plt.title(class_names[y_test[index]], fontsize=12)\n",
    "plt.subplots_adjust(wspace=0.2, hspace=0.5)\n",
    "save_fig('fashion_mnist_images_plot', tight_layout=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving and Restoring\n",
    "\n",
    "Now that we've done trained our network, we want to save the weights out for future use. We can save our model out with\n",
    "\n",
    "* `model.save(\"my_keras_model.h5\")` - Saves keras model as \"my_keras_model.h5\".\n",
    "\n",
    "We can load it back in later with\n",
    "\n",
    "* `model = keras.models.load_model(\"my_keras_model.h5\")` - Load model from saved model file. \n",
    "\n",
    "We can also checkpoint a model by saving or loading just the weights. This allows us to keep several versions of the model during training:\n",
    "\n",
    "* `model.save_weights(\"my_keras_weights.ckpt\")` - Save model weights.\n",
    "* `model.load_weights(\"my_keras_weights.ckpt\")` - Load model weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"my_keras_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"my_keras_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.load_model(\"my_keras_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict(X_valid[0,:,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights(\"my_keras_weights.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(\"my_keras_weights.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extra: Functional API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not all neural network models are simply sequential. Some may have complex topologies. Some may have multiple inputs and/or multiple outputs. For example, a Wide & Deep neural network (see [paper](https://ai.google/research/pubs/pub45413)) connects all or part of the inputs directly to the output layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the next two sections, Geron uses the California Housing Dataset included with Keras. The code below loads and scales the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "housing = fetch_california_housing()\n",
    "\n",
    "X_train_full, X_test, y_train_full, y_test = train_test_split(housing.data, housing.target, random_state=42)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train_full, y_train_full, random_state=42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_valid = scaler.transform(X_valid)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As usual, we will set a random seed for reproducibility. If Tensorflow throws an error feel free to comment out the second line. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time the network will not be strict sequential. Instead we have an __input__ layer, feeding into 30 node dense layer called __hidden1__, which in turn feeds into a 30 node dense layer called __hidden2__. This is where the network becomes non-sequential: We then concatenate both __hidden2__ and __input__ into a single layer __concat__ that contains the data of both. Finally, we the densely connect __concat__ to the single parameter output (the cost of the house). \n",
    "\n",
    "$$\n",
    "\\begin{matrix}\n",
    "&&\\textbf{hidden1}&\\rightarrow &\\textbf{hidden2} \\\\\n",
    "&\\nearrow&&&&\\searrow\\\\\n",
    "\\textbf{input} &&\\longrightarrow &&\\longrightarrow&&\\textbf{concat} &\\rightarrow \\textbf{output}\n",
    "\\end{matrix}\n",
    "$$\n",
    "\n",
    "We can see here that we are _jumping layers,_ not simply connecting each layer to the one after. This is an example of __non-sequential__ neural network. We will talk more about how different topologies can effect to the training speed and accuracy of our model, but for now we will just be playing around with a few different topologies. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ = keras.layers.Input(shape=X_train.shape[1:])\n",
    "hidden1 = keras.layers.Dense(30, activation=\"relu\")(input_)\n",
    "hidden2 = keras.layers.Dense(30, activation=\"relu\")(hidden1)\n",
    "concat = keras.layers.concatenate([input_, hidden2])\n",
    "output = keras.layers.Dense(1)(concat)\n",
    "model = keras.models.Model(inputs=[input_], outputs=[output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.compile(loss=\"mean_squared_error\", optimizer=keras.optimizers.SGD(lr=1e-3))\n",
    "history = model.fit(X_train, y_train, epochs=20,\n",
    "                    validation_data=(X_valid, y_valid))\n",
    "mse_test = model.evaluate(X_test, y_test)\n",
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if you want to send different subsets of input features through the wide or deep paths? We will send 5 features (features 0 to 4), and 6 through the deep path (features 2 to 7). Note that 3 features will go through both (features 2, 3 and 4)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise:\n",
    "\n",
    "Analyze the network constructed below and draw it's topology."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_A = keras.layers.Input(shape=[5], name=\"wide_input\")\n",
    "input_B = keras.layers.Input(shape=[6], name=\"deep_input\")\n",
    "hidden1 = keras.layers.Dense(30, activation=\"relu\")(input_B)\n",
    "hidden2 = keras.layers.Dense(30, activation=\"relu\")(hidden1)\n",
    "concat = keras.layers.concatenate([input_A, hidden2])\n",
    "output = keras.layers.Dense(1, name=\"output\")(concat)\n",
    "model = keras.models.Model(inputs=[input_A, input_B], outputs=[output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.compile(loss=\"mse\", optimizer=keras.optimizers.SGD(lr=1e-3))\n",
    "\n",
    "X_train_A, X_train_B = X_train[:, :5], X_train[:, 2:]\n",
    "X_valid_A, X_valid_B = X_valid[:, :5], X_valid[:, 2:]\n",
    "X_test_A, X_test_B = X_test[:, :5], X_test[:, 2:]\n",
    "X_new_A, X_new_B = X_test_A[:3], X_test_B[:3]\n",
    "\n",
    "history = model.fit((X_train_A, X_train_B), y_train, epochs=20,\n",
    "                    validation_data=((X_valid_A, X_valid_B), y_valid))\n",
    "mse_test = model.evaluate((X_test_A, X_test_B), y_test)\n",
    "y_pred = model.predict((X_new_A, X_new_B))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularizing:\n",
    "\n",
    "Adding an auxiliary output for regularization: We saw in the case of ridge and lasso regression that often we can regularize the model by penalizing the weights. In this case we form two outputs, one directly from the layer __hidden2__ and one from the concatenated layer __concat__. Effectively, we are training two loss functions at the same time on a different parts of the same network. This has the potential in large networks to drastically speed training, by having one loss function fixing main large layers, and another loss function fine tuning smaller, in between layers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_A = keras.layers.Input(shape=[5], name=\"wide_input\")\n",
    "input_B = keras.layers.Input(shape=[6], name=\"deep_input\")\n",
    "hidden1 = keras.layers.Dense(30, activation=\"relu\")(input_B)\n",
    "hidden2 = keras.layers.Dense(30, activation=\"relu\")(hidden1)\n",
    "concat = keras.layers.concatenate([input_A, hidden2])\n",
    "output = keras.layers.Dense(1, name=\"main_output\")(concat)\n",
    "aux_output = keras.layers.Dense(1, name=\"aux_output\")(hidden2)\n",
    "model = keras.models.Model(inputs=[input_A, input_B],\n",
    "                           outputs=[output, aux_output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=[\"mse\", \"mse\"], loss_weights=[0.9, 0.1], optimizer=keras.optimizers.SGD(lr=1e-3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "history = model.fit([X_train_A, X_train_B], [y_train, y_train], epochs=20,\n",
    "                    validation_data=([X_valid_A, X_valid_B], [y_valid, y_valid]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_loss, main_loss, aux_loss = model.evaluate(\n",
    "    [X_test_A, X_test_B], [y_test, y_test])\n",
    "y_pred_main, y_pred_aux = model.predict([X_new_A, X_new_B])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extra: Using Callbacks during Training\n",
    "\n",
    "After each epoch, keras returns a loss, accuracy, validation loss and validation accuracy, as well as other metrics we may specify. However, sometimes you want more information: a confusion matrix, an example of the output, a \"worst offenders\" list, or something else. Callback functions allow us to have Keras return information to us each epoch. \n",
    "\n",
    "For example, Keras doesn't have an inbuilt function to return the ratio of the (log of the) validation loss to the (log of the) training loss. In the code below, Geron implements a function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrintValTrainRatioCallback(keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs):\n",
    "        print(\"\\nval/train: {:.2f}\".format(logs[\"val_loss\"] / logs[\"loss\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(30, activation=\"relu\", input_shape=[8]),\n",
    "    keras.layers.Dense(30, activation=\"relu\"),\n",
    "    keras.layers.Dense(1)\n",
    "])    \n",
    "\n",
    "model.compile(loss=\"mse\", optimizer=keras.optimizers.SGD(lr=1e-3))\n",
    "checkpoint_cb = keras.callbacks.ModelCheckpoint(\"my_keras_model.h5\", save_best_only=True)\n",
    "\n",
    "val_train_ratio_cb = PrintValTrainRatioCallback()\n",
    "history = model.fit(X_train, y_train, epochs=1,\n",
    "                    validation_data=(X_valid, y_valid),\n",
    "                    callbacks=[val_train_ratio_cb])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problems:\n",
    "\n",
    "### Problem 1: \n",
    "\n",
    "Train a deep MLP on the MNIST dataset (you can load it using `keras.datasets.mnist.load_data()`. See if you can get over 98% precision. Try searching for the optimal learning rate by using the approach presented in this chapter (i.e., by growing the learning rate exponentially, plotting the loss, and finding the point where the loss shoots up). Try adding all the bells and whistlesâ€”save checkpoints, use early stopping, and plot learning curves using TensorBoard.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2:\n",
    "\n",
    "Train a deep neural network on the MRI slices dataset. Try to get demonstrably better accuracy than classifying everything as the same label. In this case, it may be helpful to _restrict_ the number of trainable parameters, sacrificing bias for reduction in variance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "nav_menu": {
   "height": "264px",
   "width": "369px"
  },
  "toc": {
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 6,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
