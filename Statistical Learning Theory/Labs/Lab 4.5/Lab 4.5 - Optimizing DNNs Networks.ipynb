{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizing DNNs\n",
    "\n",
    "This is companion code the Lecture 11, and close follows Chapter 11 of Geron. \n",
    "\n",
    "In this lab, we're going to show how to implement all of the DNN optimizations we discussed in Lecture 11 in Keras. This includes:\n",
    "\n",
    "* __Selecting Activation Functions__: SELU, ELU, Leaky ReLU, ReLu. \n",
    "* __Choosing Weight Initializations__: He for ReLU variants, LeCun for SELU and Glorot for others. \n",
    "* __Batch Normalization__\n",
    "* __Gradient Clipping__\n",
    "* __Choosing Optimizers__: Momentum, Nesterov, AdaGrad, RMSProp, Adam, Nadam and AdaMax. \n",
    "* __Regularization__: $\\ell_p$ regularization, Max-Norm, dropout and early stopping. \n",
    "\n",
    "### Note: \n",
    "\n",
    "In the notebook below I will be raining each model for 10 steps for time reasons. This is really not enough to see the differences between some of the gains as the initial model is still clearly making steady improvement at this stage. You should really train the model for at least 100 epochs. \n",
    "\n",
    "We'll start with the fashion MNIST dataset and a simple network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_3 (Flatten)          (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 300)               235500    \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 100)               30100     \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 10)                1010      \n",
      "=================================================================\n",
      "Total params: 266,610\n",
      "Trainable params: 266,610\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "fashion_mnist = keras.datasets.fashion_mnist\n",
    "(X_train_full, y_train_full), (X_test, y_test) = fashion_mnist.load_data()\n",
    "\n",
    "X_valid, X_train = X_train_full[:5000] / 255., X_train_full[5000:] / 255.\n",
    "y_valid, y_train = y_train_full[:5000], y_train_full[5000:]\n",
    "X_test = X_test / 255.\n",
    "\n",
    "class_names = [\"T-shirt/top\", \"Trouser\", \"Pullover\", \"Dress\", \"Coat\",\n",
    "               \"Sandal\", \"Shirt\", \"Sneaker\", \"Bag\", \"Ankle boot\"]\n",
    "\n",
    "\n",
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Flatten(input_shape=[28, 28]))\n",
    "model.add(keras.layers.Dense(300, activation=\"relu\"))\n",
    "model.add(keras.layers.Dense(100, activation=\"relu\"))\n",
    "model.add(keras.layers.Dense(10, activation=\"softmax\"))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 55000 samples, validate on 5000 samples\n",
      "Epoch 1/10\n",
      "55000/55000 [==============================] - 9s 155us/sample - loss: 0.7207 - accuracy: 0.7617 - val_loss: 0.5234 - val_accuracy: 0.8146\n",
      "Epoch 2/10\n",
      "55000/55000 [==============================] - 8s 145us/sample - loss: 0.4893 - accuracy: 0.8302 - val_loss: 0.4527 - val_accuracy: 0.8470\n",
      "Epoch 3/10\n",
      "55000/55000 [==============================] - 6s 115us/sample - loss: 0.4428 - accuracy: 0.8463 - val_loss: 0.4313 - val_accuracy: 0.8532\n",
      "Epoch 4/10\n",
      "55000/55000 [==============================] - 7s 124us/sample - loss: 0.4163 - accuracy: 0.8546 - val_loss: 0.4230 - val_accuracy: 0.8556\n",
      "Epoch 5/10\n",
      "55000/55000 [==============================] - 8s 140us/sample - loss: 0.3959 - accuracy: 0.8603 - val_loss: 0.3760 - val_accuracy: 0.8728\n",
      "Epoch 6/10\n",
      "55000/55000 [==============================] - 8s 139us/sample - loss: 0.3794 - accuracy: 0.8663 - val_loss: 0.3934 - val_accuracy: 0.8536\n",
      "Epoch 7/10\n",
      "55000/55000 [==============================] - 7s 132us/sample - loss: 0.3649 - accuracy: 0.8709 - val_loss: 0.3614 - val_accuracy: 0.8782\n",
      "Epoch 8/10\n",
      "55000/55000 [==============================] - 7s 123us/sample - loss: 0.3538 - accuracy: 0.8749 - val_loss: 0.3677 - val_accuracy: 0.8698\n",
      "Epoch 9/10\n",
      "55000/55000 [==============================] - 7s 134us/sample - loss: 0.3430 - accuracy: 0.8790 - val_loss: 0.3551 - val_accuracy: 0.8748\n",
      "Epoch 10/10\n",
      "55000/55000 [==============================] - 7s 126us/sample - loss: 0.3347 - accuracy: 0.8808 - val_loss: 0.3457 - val_accuracy: 0.8760\n"
     ]
    }
   ],
   "source": [
    "## Compile the model with loss: sparse_categorical_crossentropy, optimizer: sgg, and metric: accuracy. \n",
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "             optimizer=\"sgd\",\n",
    "             metrics=[\"accuracy\"])\n",
    "\n",
    "history = model.fit(X_train,y_train, epochs=10, \n",
    "          validation_data=[X_valid,y_valid])\n",
    "\n",
    "## Fit the model for 10 epochs, vailidating on the validation data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time\n",
    "\n",
    "I'm getting roughly .87 accuracy after 10 epochs, 8s per epoch for a training time of 84s.  Not bad. \n",
    "\n",
    "To time the whole output we can use the `time` library to mark the beginning and the end of our run. The `time.time()` command returns the number of seconds after January 1, 1970, 00:00:00 (UTC), the so called _epoch_. We can then measure the total time of a training run with \n",
    "\n",
    "    import time\n",
    "\n",
    "    start = time.time()\n",
    "    ## Run Training Code\n",
    "    end = time.time()\n",
    "    print(end - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation Functions\n",
    "\n",
    "Changing the activation functions using Keras is simple. In the creation of the dense layer, you just specify `activation=` and `kernel_initializer=`\n",
    "\n",
    "    keras.layers.Dense(10, activation=\"relu\", kernel_initializer=\"he_normal\")\n",
    "    \n",
    "For the full list of activations, see https://keras.io/activations/. In our case, let's change all of our layers to ELU units with He initialization. The default kernel initializer is `glorot_uniform`, or uniformally distributed weights according to the Glorot normalization. If we change the activation we should also change the initializer. The list of initializers can be found at https://keras.io/initializers/."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_5 (Flatten)          (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 300)               235500    \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 100)               30100     \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 10)                1010      \n",
      "=================================================================\n",
      "Total params: 266,610\n",
      "Trainable params: 266,610\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Flatten(input_shape=[28, 28]))\n",
    "model.add(keras.layers.Dense(300, activation=\"elu\", kernel_initializer=\"he_normal\"))\n",
    "model.add(keras.layers.Dense(100, activation=\"elu\", kernel_initializer=\"he_normal\"))\n",
    "model.add(keras.layers.Dense(10, activation=\"softmax\"))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 55000 samples, validate on 5000 samples\n",
      "Epoch 1/10\n",
      "55000/55000 [==============================] - 9s 159us/sample - loss: 0.6415 - accuracy: 0.7811 - val_loss: 0.5002 - val_accuracy: 0.8254\n",
      "Epoch 2/10\n",
      "55000/55000 [==============================] - 8s 150us/sample - loss: 0.4759 - accuracy: 0.8309 - val_loss: 0.4381 - val_accuracy: 0.8542\n",
      "Epoch 3/10\n",
      "55000/55000 [==============================] - 8s 144us/sample - loss: 0.4382 - accuracy: 0.8452 - val_loss: 0.4184 - val_accuracy: 0.8570\n",
      "Epoch 4/10\n",
      "55000/55000 [==============================] - 8s 148us/sample - loss: 0.4153 - accuracy: 0.8537 - val_loss: 0.4127 - val_accuracy: 0.8574\n",
      "Epoch 5/10\n",
      "55000/55000 [==============================] - 7s 133us/sample - loss: 0.3994 - accuracy: 0.8581 - val_loss: 0.3966 - val_accuracy: 0.8636\n",
      "Epoch 6/10\n",
      "55000/55000 [==============================] - 8s 146us/sample - loss: 0.3866 - accuracy: 0.8623 - val_loss: 0.3784 - val_accuracy: 0.8682\n",
      "Epoch 7/10\n",
      "55000/55000 [==============================] - 8s 148us/sample - loss: 0.3738 - accuracy: 0.8667 - val_loss: 0.3746 - val_accuracy: 0.8676\n",
      "Epoch 8/10\n",
      "55000/55000 [==============================] - 8s 148us/sample - loss: 0.3645 - accuracy: 0.8694 - val_loss: 0.3759 - val_accuracy: 0.8708\n",
      "Epoch 9/10\n",
      "55000/55000 [==============================] - 8s 150us/sample - loss: 0.3563 - accuracy: 0.8726 - val_loss: 0.3673 - val_accuracy: 0.8674\n",
      "Epoch 10/10\n",
      "55000/55000 [==============================] - 8s 149us/sample - loss: 0.3479 - accuracy: 0.8756 - val_loss: 0.3479 - val_accuracy: 0.8752\n",
      "Training Time: 81.31610941886902\n"
     ]
    }
   ],
   "source": [
    "## Compile the model with loss: sparse_categorical_crossentropy, optimizer: sgg, and metric: accuracy. \n",
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "             optimizer=\"sgd\",\n",
    "             metrics=[\"accuracy\"])\n",
    "\n",
    "import time\n",
    "start = time.time()\n",
    "\n",
    "history = model.fit(X_train,y_train, epochs=10, \n",
    "          validation_data=[X_valid,y_valid])\n",
    "\n",
    "## Fit the model for 10 epochs, vailidating on the validation data. \n",
    "\n",
    "end = time.time()\n",
    "print(\"Training Time:\",end - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case we're getting slightly better results with the original ReLU function, but try training longer and see what you find. We're already doing fairly well here so we might not see too many gains. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Normalization\n",
    "\n",
    "Recall that Batch Normalization adds a layer before each dense layer that normalizes the input data. In Keras implementing batch normalization works exactly as you might expect, by adding a `keras.layers.BatchNormalization()` before any sequential layer we want to normalize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_6 (Flatten)          (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 784)               3136      \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 300)               235500    \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 300)               1200      \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 100)               30100     \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 10)                1010      \n",
      "=================================================================\n",
      "Total params: 271,346\n",
      "Trainable params: 268,978\n",
      "Non-trainable params: 2,368\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Flatten(input_shape=[28, 28]))\n",
    "model.add(keras.layers.BatchNormalization())\n",
    "model.add(keras.layers.Dense(300, activation=\"elu\", kernel_initializer=\"he_normal\"))\n",
    "model.add(keras.layers.BatchNormalization())\n",
    "model.add(keras.layers.Dense(100, activation=\"elu\", kernel_initializer=\"he_normal\"))\n",
    "model.add(keras.layers.BatchNormalization())\n",
    "model.add(keras.layers.Dense(10, activation=\"softmax\"))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the number of parameters has gone up. This is because each batch normalization layer has a number of trainable parameters governing the trained center and shape. Lets take a quick look at a batch normalization layer. Using the `model.layers` list, we will access the variables from the first layer and take a look at the 3136 variables inside:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('batch_normalization/gamma:0', True),\n",
       " ('batch_normalization/beta:0', True),\n",
       " ('batch_normalization/moving_mean:0', False),\n",
       " ('batch_normalization/moving_variance:0', False)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(var.name, var.trainable) for var in model.layers[1].variables]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that there are two trainable parameters, the center `gamma` and the scale `beta` and two parameters computed from the batch, the `moving_mean` $\\hat{\\mu}$ and the `moving_variance` $\\hat{\\sigma}^2$. The last two parameters are the moving average of the batch means and variances, to be used at prediction time. There are a lot of little hyperparameters to play with when using batch normalization, but usually the defaults can be used. \n",
    "\n",
    "A final note: batch normalization tries to properly center the data so a bias term would be redundant. Deep layers be default contain a bias term, so you may want to turn if off using `use_bias=False`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_11\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_11 (Flatten)         (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_13 (Batc (None, 784)               3136      \n",
      "_________________________________________________________________\n",
      "dense_31 (Dense)             (None, 300)               235200    \n",
      "_________________________________________________________________\n",
      "batch_normalization_14 (Batc (None, 300)               1200      \n",
      "_________________________________________________________________\n",
      "dense_32 (Dense)             (None, 100)               30000     \n",
      "_________________________________________________________________\n",
      "batch_normalization_15 (Batc (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "dense_33 (Dense)             (None, 10)                1010      \n",
      "=================================================================\n",
      "Total params: 270,946\n",
      "Trainable params: 268,578\n",
      "Non-trainable params: 2,368\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Flatten(input_shape=[28, 28]))\n",
    "model.add(keras.layers.BatchNormalization())\n",
    "model.add(keras.layers.Dense(300, activation=\"elu\", kernel_initializer=\"he_normal\",use_bias=False))\n",
    "model.add(keras.layers.BatchNormalization())\n",
    "model.add(keras.layers.Dense(100, activation=\"elu\", kernel_initializer=\"he_normal\",use_bias=False))\n",
    "model.add(keras.layers.BatchNormalization())\n",
    "model.add(keras.layers.Dense(10, activation=\"softmax\"))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 55000 samples, validate on 5000 samples\n",
      "Epoch 1/10\n",
      "55000/55000 [==============================] - 14s 259us/sample - loss: 0.5551 - accuracy: 0.8046 - val_loss: 0.4026 - val_accuracy: 0.8636\n",
      "Epoch 2/10\n",
      "55000/55000 [==============================] - 12s 225us/sample - loss: 0.4234 - accuracy: 0.8491 - val_loss: 0.3760 - val_accuracy: 0.8718\n",
      "Epoch 3/10\n",
      "55000/55000 [==============================] - 13s 229us/sample - loss: 0.3865 - accuracy: 0.8623 - val_loss: 0.3542 - val_accuracy: 0.8722\n",
      "Epoch 4/10\n",
      "55000/55000 [==============================] - 13s 228us/sample - loss: 0.3608 - accuracy: 0.8711 - val_loss: 0.3465 - val_accuracy: 0.8772\n",
      "Epoch 5/10\n",
      "55000/55000 [==============================] - 12s 227us/sample - loss: 0.3471 - accuracy: 0.8744 - val_loss: 0.3350 - val_accuracy: 0.8782\n",
      "Epoch 6/10\n",
      "55000/55000 [==============================] - 13s 228us/sample - loss: 0.3316 - accuracy: 0.8805 - val_loss: 0.3273 - val_accuracy: 0.8804\n",
      "Epoch 7/10\n",
      "55000/55000 [==============================] - 12s 226us/sample - loss: 0.3185 - accuracy: 0.8856 - val_loss: 0.3207 - val_accuracy: 0.8854\n",
      "Epoch 8/10\n",
      "55000/55000 [==============================] - 11s 206us/sample - loss: 0.3067 - accuracy: 0.8874 - val_loss: 0.3198 - val_accuracy: 0.8856\n",
      "Epoch 9/10\n",
      "55000/55000 [==============================] - 12s 222us/sample - loss: 0.2975 - accuracy: 0.8920 - val_loss: 0.3121 - val_accuracy: 0.8866\n",
      "Epoch 10/10\n",
      "55000/55000 [==============================] - 13s 235us/sample - loss: 0.2890 - accuracy: 0.8951 - val_loss: 0.3118 - val_accuracy: 0.8862\n",
      "Training Time: 125.75716733932495\n"
     ]
    }
   ],
   "source": [
    "## Compile the model with loss: sparse_categorical_crossentropy, optimizer: sgg, and metric: accuracy. \n",
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "             optimizer=\"sgd\",\n",
    "             metrics=[\"accuracy\"])\n",
    "\n",
    "import time\n",
    "start = time.time()\n",
    "\n",
    "history = model.fit(X_train,y_train, epochs=10, \n",
    "          validation_data=[X_valid,y_valid])\n",
    "\n",
    "## Fit the model for 10 epochs, vailidating on the validation data. \n",
    "\n",
    "end = time.time()\n",
    "print(\"Training Time:\",end - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It has also been argued that bias layers should be added before the activation layer, so that the data is properly centered and scaled before activation. To do this, remove the activation call from the dense layers and add a `keras.layers.Activation(\"elu\")` layer after each batch normalization layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_12\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_12 (Flatten)         (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_16 (Batc (None, 784)               3136      \n",
      "_________________________________________________________________\n",
      "dense_34 (Dense)             (None, 300)               235200    \n",
      "_________________________________________________________________\n",
      "batch_normalization_17 (Batc (None, 300)               1200      \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "dense_35 (Dense)             (None, 100)               30000     \n",
      "_________________________________________________________________\n",
      "batch_normalization_18 (Batc (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_36 (Dense)             (None, 10)                1010      \n",
      "=================================================================\n",
      "Total params: 270,946\n",
      "Trainable params: 268,578\n",
      "Non-trainable params: 2,368\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Flatten(input_shape=[28, 28]))\n",
    "model.add(keras.layers.BatchNormalization())\n",
    "model.add(keras.layers.Dense(300, kernel_initializer=\"he_normal\",use_bias=False))\n",
    "model.add(keras.layers.BatchNormalization())\n",
    "model.add(keras.layers.Activation(\"elu\"))\n",
    "model.add(keras.layers.Dense(100, kernel_initializer=\"he_normal\",use_bias=False))\n",
    "model.add(keras.layers.BatchNormalization())\n",
    "model.add(keras.layers.Activation(\"elu\"))\n",
    "model.add(keras.layers.Dense(10, activation=\"softmax\"))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 55000 samples, validate on 5000 samples\n",
      "Epoch 1/10\n",
      "55000/55000 [==============================] - 15s 266us/sample - loss: 0.5745 - accuracy: 0.8019 - val_loss: 0.4278 - val_accuracy: 0.8562\n",
      "Epoch 2/10\n",
      "55000/55000 [==============================] - 12s 226us/sample - loss: 0.4409 - accuracy: 0.8455 - val_loss: 0.3936 - val_accuracy: 0.8654\n",
      "Epoch 3/10\n",
      "55000/55000 [==============================] - 13s 229us/sample - loss: 0.4078 - accuracy: 0.8564 - val_loss: 0.3758 - val_accuracy: 0.8698\n",
      "Epoch 4/10\n",
      "55000/55000 [==============================] - 13s 237us/sample - loss: 0.3794 - accuracy: 0.8647 - val_loss: 0.3661 - val_accuracy: 0.8710\n",
      "Epoch 5/10\n",
      "55000/55000 [==============================] - 12s 225us/sample - loss: 0.3662 - accuracy: 0.8695 - val_loss: 0.3615 - val_accuracy: 0.8698\n",
      "Epoch 6/10\n",
      "55000/55000 [==============================] - 12s 220us/sample - loss: 0.3482 - accuracy: 0.8746 - val_loss: 0.3598 - val_accuracy: 0.8712\n",
      "Epoch 7/10\n",
      "55000/55000 [==============================] - 13s 229us/sample - loss: 0.3365 - accuracy: 0.8793 - val_loss: 0.3419 - val_accuracy: 0.8826\n",
      "Epoch 8/10\n",
      "55000/55000 [==============================] - 13s 230us/sample - loss: 0.3245 - accuracy: 0.8843 - val_loss: 0.3326 - val_accuracy: 0.8758\n",
      "Epoch 9/10\n",
      "55000/55000 [==============================] - 12s 222us/sample - loss: 0.3128 - accuracy: 0.8874 - val_loss: 0.3296 - val_accuracy: 0.8824\n",
      "Epoch 10/10\n",
      "55000/55000 [==============================] - 11s 200us/sample - loss: 0.3043 - accuracy: 0.8899 - val_loss: 0.3339 - val_accuracy: 0.8788\n",
      "Training Time: 125.7387306690216\n"
     ]
    }
   ],
   "source": [
    "## Compile the model with loss: sparse_categorical_crossentropy, optimizer: sgg, and metric: accuracy. \n",
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "             optimizer=\"sgd\",\n",
    "             metrics=[\"accuracy\"])\n",
    "\n",
    "import time\n",
    "start = time.time()\n",
    "\n",
    "history = model.fit(X_train,y_train, epochs=10, \n",
    "          validation_data=[X_valid,y_valid])\n",
    "\n",
    "## Fit the model for 10 epochs, vailidating on the validation data. \n",
    "\n",
    "end = time.time()\n",
    "print(\"Training Time:\",end - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that our training is taking longer due to the extra steps added, but we are rewarded with a higher validation accuracy. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Clipping and Optimizers\n",
    "\n",
    "Gradient clipping is done at the optimizer level by setting the `clipvalue` to your desired value. For example, to use gradient clipping with SGD we use\n",
    "\n",
    "    optimizer = keras.optimizers.SGD(clipvalue=1.0)\n",
    "    model.compile(loss=\"mse\", optimizer=optimizer)\n",
    "    \n",
    "#### Momentum Optimization and Nesterov Accelerated Gradient\n",
    "\n",
    "Momentum optimization can be similarly used by setting the `momentum = ` value in most optimizers to be nonzero. For example, to enable momentum optimization in SGD we use\n",
    "\n",
    "    optimizer = keras.optimizers.SGD(lr=0.001, momentum=0.9)\n",
    "    \n",
    "Similarly, to use the Nesterov trick we pass `nesterov = True` to the optimizer: \n",
    "\n",
    "    optimizer = keras.optimizers.SGD(lr=0.001, momentum=0.9, nesterov=True)\n",
    "    \n",
    "#### RMSProp and Adam\n",
    "\n",
    "RMSProp and Adam are build in keras optimizers and can be initialized using \n",
    "\n",
    "    optimizer = keras.optimizers.RMSprop(lr=0.001, rho=0.9)\n",
    " \n",
    "and\n",
    "\n",
    "    optimizer = keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999)\n",
    "    \n",
    "For Adam, `beta_1` is the momentum falloff rate and `beta_2` is the scale falloff rate.\n",
    "\n",
    "Nadam and AdaMax are also implemented: https://keras.io/optimizers/.  \n",
    "\n",
    "\n",
    "### Implementing\n",
    "\n",
    "To use a different optimizer, and to be able to specify it's parameters, we just change the compile line \n",
    "\n",
    "    model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"sgd\", metrics=[\"accuracy\"])\n",
    "\n",
    "to take an initialized optimizer:\n",
    "\n",
    "    optimizer = keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999)\n",
    "    model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\"])\n",
    "\n",
    "The code below implements the Adam optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_7 (Flatten)          (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 784)               3136      \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 300)               235500    \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 300)               1200      \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 100)               30100     \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 10)                1010      \n",
      "=================================================================\n",
      "Total params: 271,346\n",
      "Trainable params: 268,978\n",
      "Non-trainable params: 2,368\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Flatten(input_shape=[28, 28]))\n",
    "model.add(keras.layers.BatchNormalization())\n",
    "model.add(keras.layers.Dense(300, activation=\"elu\", kernel_initializer=\"he_normal\"))\n",
    "model.add(keras.layers.BatchNormalization())\n",
    "model.add(keras.layers.Dense(100, activation=\"elu\", kernel_initializer=\"he_normal\"))\n",
    "model.add(keras.layers.BatchNormalization())\n",
    "model.add(keras.layers.Dense(10, activation=\"softmax\"))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 55000 samples, validate on 5000 samples\n",
      "Epoch 1/10\n",
      "55000/55000 [==============================] - 15s 271us/sample - loss: 0.4705 - accuracy: 0.8301 - val_loss: 0.3529 - val_accuracy: 0.8762\n",
      "Epoch 2/10\n",
      "55000/55000 [==============================] - 13s 239us/sample - loss: 0.3640 - accuracy: 0.8664 - val_loss: 0.3182 - val_accuracy: 0.8832\n",
      "Epoch 3/10\n",
      "55000/55000 [==============================] - 13s 244us/sample - loss: 0.3222 - accuracy: 0.8816 - val_loss: 0.3040 - val_accuracy: 0.8880\n",
      "Epoch 4/10\n",
      "55000/55000 [==============================] - 13s 236us/sample - loss: 0.2979 - accuracy: 0.8891 - val_loss: 0.2991 - val_accuracy: 0.8912\n",
      "Epoch 5/10\n",
      "55000/55000 [==============================] - 13s 229us/sample - loss: 0.2765 - accuracy: 0.8983 - val_loss: 0.2921 - val_accuracy: 0.8918\n",
      "Epoch 6/10\n",
      "55000/55000 [==============================] - 11s 206us/sample - loss: 0.2602 - accuracy: 0.9024 - val_loss: 0.2973 - val_accuracy: 0.8886\n",
      "Epoch 7/10\n",
      "55000/55000 [==============================] - 13s 238us/sample - loss: 0.2463 - accuracy: 0.9083 - val_loss: 0.2954 - val_accuracy: 0.8934\n",
      "Epoch 8/10\n",
      "55000/55000 [==============================] - 13s 228us/sample - loss: 0.2293 - accuracy: 0.9140 - val_loss: 0.2989 - val_accuracy: 0.8884\n",
      "Epoch 9/10\n",
      "55000/55000 [==============================] - 13s 231us/sample - loss: 0.2173 - accuracy: 0.9175 - val_loss: 0.2822 - val_accuracy: 0.9040\n",
      "Epoch 10/10\n",
      "55000/55000 [==============================] - 12s 227us/sample - loss: 0.2083 - accuracy: 0.9214 - val_loss: 0.3013 - val_accuracy: 0.8964\n",
      "Training Time: 129.37052965164185\n"
     ]
    }
   ],
   "source": [
    "## Compile the model with loss: sparse_categorical_crossentropy, optimizer: Adam, and metric: accuracy. \n",
    "\n",
    "optimizer = keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999)\n",
    "\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "             optimizer=optimizer,\n",
    "             metrics=[\"accuracy\"])\n",
    "\n",
    "import time\n",
    "start = time.time()\n",
    "\n",
    "history = model.fit(X_train,y_train, epochs=10, \n",
    "          validation_data=[X_valid,y_valid])\n",
    "\n",
    "## Fit the model for 10 epochs, vailidating on the validation data. \n",
    "\n",
    "end = time.time()\n",
    "print(\"Training Time:\",end - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just by changing the optimizer we're above 90% accuracy, and with no appreciable difference in training time.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding Regularization\n",
    "\n",
    "Regularization is done at the layer level, allowing us to choose which layers we regularize if we with. To turn on regularization, pass a `kernal_regularizer = ` to the layer creation function\n",
    "\n",
    "    layer = keras.layers.Dense(100, activation=\"elu\",\n",
    "        kernel_initializer=\"he_normal\",\n",
    "        kernel_regularizer=keras.regularizers.l2(0.01))\n",
    "        \n",
    "For $\\ell_1$ of mixed $\\ell_1-\\ell_2$ regularization use `keras.regularizers.l1` or `keras.regularizers.l1_l2`. For Max-Norm regularization use `keras.constraints.max_norm(1.)`. The full list of regularizers can be found here: https://keras.io/regularizers/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_13\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_13 (Flatten)         (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_19 (Batc (None, 784)               3136      \n",
      "_________________________________________________________________\n",
      "dense_37 (Dense)             (None, 300)               235500    \n",
      "_________________________________________________________________\n",
      "batch_normalization_20 (Batc (None, 300)               1200      \n",
      "_________________________________________________________________\n",
      "dense_38 (Dense)             (None, 100)               30100     \n",
      "_________________________________________________________________\n",
      "batch_normalization_21 (Batc (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "dense_39 (Dense)             (None, 10)                1010      \n",
      "=================================================================\n",
      "Total params: 271,346\n",
      "Trainable params: 268,978\n",
      "Non-trainable params: 2,368\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Flatten(input_shape=[28, 28]))\n",
    "model.add(keras.layers.BatchNormalization())\n",
    "model.add(keras.layers.Dense(300, activation=\"elu\", kernel_initializer=\"he_normal\",kernel_regularizer=keras.regularizers.l2(0.01)))\n",
    "model.add(keras.layers.BatchNormalization())\n",
    "model.add(keras.layers.Dense(100, activation=\"elu\", kernel_initializer=\"he_normal\",kernel_regularizer=keras.regularizers.l2(0.01)))\n",
    "model.add(keras.layers.BatchNormalization())\n",
    "model.add(keras.layers.Dense(10, activation=\"softmax\",kernel_regularizer=keras.regularizers.l2(0.01)))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 55000 samples, validate on 5000 samples\n",
      "Epoch 1/10\n",
      "55000/55000 [==============================] - 17s 309us/sample - loss: 1.4729 - accuracy: 0.7901 - val_loss: 0.7201 - val_accuracy: 0.8258\n",
      "Epoch 2/10\n",
      "55000/55000 [==============================] - 13s 237us/sample - loss: 0.7462 - accuracy: 0.8016 - val_loss: 0.6652 - val_accuracy: 0.8346\n",
      "Epoch 3/10\n",
      "55000/55000 [==============================] - 12s 224us/sample - loss: 0.6774 - accuracy: 0.8170 - val_loss: 0.5854 - val_accuracy: 0.8522\n",
      "Epoch 4/10\n",
      "55000/55000 [==============================] - 13s 234us/sample - loss: 0.6243 - accuracy: 0.8261 - val_loss: 0.5939 - val_accuracy: 0.8350\n",
      "Epoch 5/10\n",
      "55000/55000 [==============================] - 12s 216us/sample - loss: 0.5938 - accuracy: 0.8329 - val_loss: 0.5447 - val_accuracy: 0.8528\n",
      "Epoch 6/10\n",
      "55000/55000 [==============================] - 12s 218us/sample - loss: 0.5737 - accuracy: 0.8363 - val_loss: 0.5559 - val_accuracy: 0.8404\n",
      "Epoch 7/10\n",
      "55000/55000 [==============================] - 12s 225us/sample - loss: 0.5639 - accuracy: 0.8370 - val_loss: 0.5230 - val_accuracy: 0.8554\n",
      "Epoch 8/10\n",
      "55000/55000 [==============================] - 12s 210us/sample - loss: 0.5580 - accuracy: 0.8355 - val_loss: 0.5456 - val_accuracy: 0.8410\n",
      "Epoch 9/10\n",
      "55000/55000 [==============================] - 11s 208us/sample - loss: 0.5532 - accuracy: 0.8392 - val_loss: 0.5131 - val_accuracy: 0.8504\n",
      "Epoch 10/10\n",
      "55000/55000 [==============================] - 12s 225us/sample - loss: 0.5467 - accuracy: 0.8404 - val_loss: 0.4819 - val_accuracy: 0.8590\n",
      "Training Time: 126.95995712280273\n"
     ]
    }
   ],
   "source": [
    "## Compile the model with loss: sparse_categorical_crossentropy, optimizer: Adam, and metric: accuracy. \n",
    "\n",
    "optimizer = keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999)\n",
    "\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "             optimizer=optimizer,\n",
    "             metrics=[\"accuracy\"])\n",
    "\n",
    "import time\n",
    "start = time.time()\n",
    "\n",
    "history = model.fit(X_train,y_train, epochs=10, \n",
    "          validation_data=[X_valid,y_valid])\n",
    "\n",
    "## Fit the model for 10 epochs, vailidating on the validation data. \n",
    "\n",
    "end = time.time()\n",
    "print(\"Training Time:\",end - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regularization is slowing down the training which is perhaps unsurprising , but it seems have really punished us for no particular gain in accuracy. We may be regularizing too much, but it could also be that while it takes longer to get where its going, the regularized classifier is more accurate on new data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropout\n",
    "\n",
    "To add dropout to the layers of the network, we simply need to add a dropout layer before any dense layers:\n",
    "\n",
    "    keras.layers.Dropout(rate=0.2)\n",
    "    \n",
    "Dropout tends to significantly slow convergence, but the results are much better if you have the training time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_14\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_14 (Flatten)         (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_40 (Dense)             (None, 300)               235500    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "dense_41 (Dense)             (None, 100)               30100     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_42 (Dense)             (None, 10)                1010      \n",
      "=================================================================\n",
      "Total params: 266,610\n",
      "Trainable params: 266,610\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Flatten(input_shape=[28, 28]))\n",
    "model.add(keras.layers.Dropout(rate=0.2))\n",
    "model.add(keras.layers.Dense(300, activation=\"elu\", kernel_initializer=\"he_normal\",kernel_regularizer=keras.regularizers.l2(0.01)))\n",
    "model.add(keras.layers.Dropout(rate=0.2))\n",
    "model.add(keras.layers.Dense(100, activation=\"elu\", kernel_initializer=\"he_normal\",kernel_regularizer=keras.regularizers.l2(0.01)))\n",
    "model.add(keras.layers.Dropout(rate=0.2))\n",
    "model.add(keras.layers.Dense(10, activation=\"softmax\",kernel_regularizer=keras.regularizers.l2(0.01)))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 55000 samples, validate on 5000 samples\n",
      "Epoch 1/10\n",
      "55000/55000 [==============================] - 6s 114us/sample - loss: 1.6822 - accuracy: 0.7522 - val_loss: 0.9546 - val_accuracy: 0.7970\n",
      "Epoch 2/10\n",
      "55000/55000 [==============================] - 6s 113us/sample - loss: 1.0104 - accuracy: 0.7601 - val_loss: 0.9187 - val_accuracy: 0.8046\n",
      "Epoch 3/10\n",
      "55000/55000 [==============================] - 6s 110us/sample - loss: 0.9931 - accuracy: 0.7629 - val_loss: 0.9213 - val_accuracy: 0.7918\n",
      "Epoch 4/10\n",
      "55000/55000 [==============================] - 8s 146us/sample - loss: 0.9893 - accuracy: 0.7661 - val_loss: 0.9187 - val_accuracy: 0.7940\n",
      "Epoch 5/10\n",
      "55000/55000 [==============================] - 8s 145us/sample - loss: 0.9851 - accuracy: 0.7632 - val_loss: 0.9307 - val_accuracy: 0.7922\n",
      "Epoch 6/10\n",
      "55000/55000 [==============================] - 10s 177us/sample - loss: 0.9798 - accuracy: 0.7656 - val_loss: 0.9197 - val_accuracy: 0.7928\n",
      "Epoch 7/10\n",
      "55000/55000 [==============================] - 6s 113us/sample - loss: 0.9807 - accuracy: 0.7666 - val_loss: 0.9096 - val_accuracy: 0.7930\n",
      "Epoch 8/10\n",
      "55000/55000 [==============================] - 6s 104us/sample - loss: 0.9822 - accuracy: 0.7643 - val_loss: 0.9279 - val_accuracy: 0.7786\n",
      "Epoch 9/10\n",
      "55000/55000 [==============================] - 6s 103us/sample - loss: 0.9770 - accuracy: 0.7664 - val_loss: 0.9133 - val_accuracy: 0.7924\n",
      "Epoch 10/10\n",
      "55000/55000 [==============================] - 6s 104us/sample - loss: 0.9736 - accuracy: 0.7675 - val_loss: 0.8940 - val_accuracy: 0.7966\n",
      "Training Time: 67.59742403030396\n"
     ]
    }
   ],
   "source": [
    "## Compile the model with loss: sparse_categorical_crossentropy, optimizer: Adam, and metric: accuracy. \n",
    "\n",
    "optimizer = keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999)\n",
    "\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "             optimizer=optimizer,\n",
    "             metrics=[\"accuracy\"])\n",
    "\n",
    "import time\n",
    "start = time.time()\n",
    "\n",
    "history = model.fit(X_train,y_train, epochs=10, \n",
    "          validation_data=[X_valid,y_valid])\n",
    "\n",
    "## Fit the model for 10 epochs, vailidating on the validation data. \n",
    "\n",
    "end = time.time()\n",
    "print(\"Training Time:\",end - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question:\n",
    "\n",
    "What about using dropout and batch normalization together? Does the order matter? For example, we could use\n",
    "\n",
    "Dense -> Batch Norm -> Activation -> Dropout -> Dense\n",
    "\n",
    "as the original paper suggests, but recent test suggest this is actually not the optimal order. Perform some test to try to determine for the network above which is the proper ordering. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
