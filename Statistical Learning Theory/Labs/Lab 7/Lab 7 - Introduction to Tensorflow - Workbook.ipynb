{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Deeper into Keras with TensorFlow \n",
    "\n",
    "TensorFlow is Google's machine learning programing language. TensorFlow is a very general tools for constructing artificial neural networks, both as operation trees and as more topologically complicated networks. TensorFlow's nodes incorporate all of the structure we discussed in the lecture, including most importantly the reverse autodiff partial derivative information.\n",
    "\n",
    "It's important to remember that using TensorFlow for computations proceeds in two parts:\n",
    "\n",
    "1. __Construct a graph:__ In this step, create a pattern for the network by defining all of our nodes (input, output, constant, multipliers, LSUs, etc) and connecting them. You also specify their initial values. At this stage no construction has been done. \n",
    "\n",
    "2. __Compile the model:__ To actually put the model in memory you must compile it. \n",
    "\n",
    "3. __Fit or Run the Graph:__  Fit the graph, and when you are satisfied save out the weights. \n",
    "\n",
    "<img src = \"https://www.tensorflow.org/images/tensors_flowing.gif\">\n",
    "\n",
    "Higher level tools like Keras hide the details of the initialization and session and streamline this process by including pre-made versions of common architectures. This lab will focus on using the low level tools to enhance Tensorflow. This lab is meant to work with Tensorflow 2.1.\n",
    "\n",
    "\n",
    "#### Getting TensorFlow and Keras\n",
    "\n",
    "If you are on Google Colab, TensorFlow and Keras are automatically installed. If you are running Jupyter locally use the anaconda prompt to install using \n",
    "\n",
    "    $ pip install tensorflow\n",
    "$ pip install keras\n",
    "\n",
    "* Windows: Open \"Anaconda Prompt\" from the start menu or from Cortana's search. \n",
    "* OSX: Open terminal, you should directly be able to use pip from there. \n",
    "* Linux: Open terminal, you should directly be able to use pip from there. \n",
    "\n",
    "For more information, see https://www.tensorflow.org/install and https://keras.io/#installation.\n",
    "\n",
    "This lab follows Chapters 9 and 10 from *Hands-On Machine Learning with Scikit_Learn & TensorFlow*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading TensorFlow\n",
    "\n",
    "After TensorFlow is installed restart your kernel. The code below loads Tensorflow, Keras and then sets the random seeds for numpy. This makes your results consistent across runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Operations with TensorFlow\n",
    "\n",
    "For an excelent post about the annatomy of a tensor see https://pgaleone.eu/tensorflow/2018/07/28/understanding-tensorflow-tensors-shape-static-dynamic/\n",
    "\n",
    "We will start by making the simple operation tree below. Our first step is to set up the graph. We will define TensorFlow variables using \n",
    "\n",
    "* `tf.constant(input, name=\"VarName\")` Creates a TensorFlow array from the input data `input`. Name it is optional, but will make it easier to call within TensorFlow. \n",
    "\n",
    "Tensorflows variables and constants act much like `numpy` objects. They have a values, a shape and a datatype. For example, lets define  \n",
    "$$\n",
    "X^2Y + Y -4\\,,\n",
    "$$\n",
    "initialized so that $X=3$ and $Y = 2$ we can use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.constant(3, name=\"X\")\n",
    "Y = tf.constant(2, name=\"Y\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output for `f` is a `tf.Tensor` object. Lets take a look at it:\n",
    "\n",
    "* `id=` - The nodes/tensors unique ID.\n",
    "* `shape=()` - The shape is number of elements in each dimension. A scaler has rank 0 and an empty shape, a vector has rank 1 and a shape (N), where N is the number of elements in the vector. A matrix has rank 2 and shape (N_1, N_2). \n",
    "* `numpy` - The content of the tensor as a numpy array. \n",
    "\n",
    "Lets look at an example with higher order arrays. We can multiply matrices just like in numpy, with `tf.multiply`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a dataflow graph.\n",
    "c = tf.constant([[1.0, 2.0], [3.0, 4.0]])\n",
    "d = tf.constant([[1.0, 1.0], [0.0, 1.0]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we see that the shape is (2,2), the data type is a `float32` and the content of the tensor as a numpy object would be an array of floats\n",
    "\n",
    "    array([[1., 3.],[3., 7.]], dtype=float32))\n",
    "    \n",
    "We should leave Tensorflow objects as Tensorflow objects, but we can quarry as `tf.Tensor` object to return any of these parameters:\n",
    "\n",
    "* `tf.Tensor.numpy()` - Return contents as an numpy array.\n",
    "* `tf.Tensor.shape` - View the tensors shape parameter.\n",
    "* `tf.Tensor.dtype` - View the tensors datatype parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Indexing for tf.Tensor Objects\n",
    "\n",
    "Indexing works much the same as for numpy arrays:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensors can be manipulated like numpy objects, provided you use the proper tensorflow commands to perform thing like transposing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(c+10)\n",
    "display(tf.transpose(c))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a partial list of tensorflow operations:\n",
    "\n",
    "* `tf.add()`\n",
    "* `tf.multiply()`\n",
    "* `tf.square()`\n",
    "* `tf.exp()`\n",
    "* `tf.sqrt()`\n",
    "* `tf.reshape()`\n",
    "* `tf.squeeze()`\n",
    "* `tf.tile())`\n",
    "\n",
    "Some functions have a different name than their numpy equivalent. For example, \n",
    "\n",
    "* `tf.reduce_mean()` - Return the mean of the array.\n",
    "* `tf.reduce_sum()` - Return the summation of elements in the array. \n",
    "* `tf.reduce_max()` - Return the max of the elements in the array.\n",
    "* `tf.math.log()` - Return log of the array. \n",
    "\n",
    "Tensorflow uses _reduce_ here because the algorithm used at the GPU level is a _reduce_ algorithm that does not guarantee the order in which elements are added. Finally, matrix operations can be performed with\n",
    "\n",
    "* `tf.transpose()` - Returns a new tensor that is a transpose of the old tensor.\n",
    "* `tf.matmul()` - Returns a new tensor that is a matrix multiplication of two tensors. \n",
    "* `tf.linalg.inv()` - Returns the inverted matrix. \n",
    "\n",
    "For a more complete list of Tensorflow's matheamtics operations see the documentation for `tf.math`:\n",
    "\n",
    "https://www.tensorflow.org/api_docs/python/tf/math\n",
    "\n",
    "and `tf.linalg`:\n",
    "\n",
    "https://www.tensorflow.org/api_docs/python/tf/linalg\n",
    "\n",
    "\n",
    "__Note:__ In Tensorflow  `M @ N` is __matrix multiplication__ while `M*N` is componentwise multiplication:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, Tensorflow plays nicely with numpy and will allow many numpy functions to be apply to `tf.Tensor` objects:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.square(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Strings:\n",
    "\n",
    "It can handle strings and list of strings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Strings will be stored as bytes encoded for unicode:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.strings.unicode_decode(p, \"UTF8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tf.strings library has a large list of text processing tools that can be found at \n",
    "\n",
    "https://www.tensorflow.org/api_docs/python/tf/strings\n",
    "\n",
    "For example, we can get the string lengths, or join the list into a single string:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variables\n",
    "\n",
    "The constants we've been working with cannot be changed and any operations done to them are returned as something new. However, for a neural network we want both trainable and untrainable parameters so we will need `tf.Variable` objects. \n",
    "\n",
    "The `v = tf.Variable` objects can be updated in place using \n",
    "\n",
    "* `v.assign(new)` - Update the value of the variable `v` to the value `new`.\n",
    "* `v.assign_add(N)` - Increment the variable by `N`. \n",
    "* `v.assign_add(N)` - Decrement the variable by `N`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v = tf.Variable([[1., 2., 3.], [4., 5., 6.]])\n",
    "v.assign(2*v)\n",
    "display(v)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise:\n",
    "\n",
    "Lets use TensorFlow compute our linear regression solution $\\beta = (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{y}$. Try to construct the matrix beta below for the breast cancer data set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "cancer = load_breast_cancer()\n",
    "m,n = cancer.data.shape\n",
    "\n",
    "cancer_bias = np.c_[np.ones((m,1)),cancer.data]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "Use Tensorflow to implement Elastic Net Regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Customizing Models with Tensorflow:\n",
    "\n",
    "Tensorflow is a library under development, and as a result not every function is implemented yet. In addition, for a specific application you may come up with your own loss function, or may want to perform tests comparing different kinds of homebrewed activation function. In these cases, we will use the lower level tensorflow to construct new functions. \n",
    "\n",
    "Lets implement the [Huber Loss function](https://en.wikipedia.org/wiki/Huber_loss)\n",
    "\n",
    "$$\n",
    "L_\\delta (a) = \\begin{cases}\n",
    " \\frac{1}{2}{a^2}                   & \\text{for } |a| \\le \\delta, \\\\\n",
    " \\delta (|a| - \\frac{1}{2}\\delta), & \\text{otherwise.}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "In the graph below, the Huber loss is in green and the squared error loss is in blue. \n",
    "\n",
    "<img width = 400 src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/c/cc/Huber_loss.svg/600px-Huber_loss.svg.png\">\n",
    "\n",
    "The Huber loss is used in robust regression and is less sensitive to outliers and noisy data than the squared error loss. \n",
    "\n",
    "In this case, we'll use $\\delta=1$. It's clear that our program flow should be roughly something like\n",
    "\n",
    "__(Psudocode)__:\n",
    "\n",
    "    ERROR = y_true - y_pred\n",
    "    if (ERROR > 1) then\n",
    "        return abs(ERROR) - .5\n",
    "    else\n",
    "        return (ERROR^2)/2\n",
    "\n",
    "However, for speed reasons we want to do everything inside Tensorflow constructing each step as a node in a graph, that includes the if statement. Tensorflow has an if node in the form of `tf.where`:\n",
    "\n",
    "* `tf.where(is_true, V1, V2)` - If the tensor `is_true` contains `True` then return `V1`, otherwise return `V2`. \n",
    "\n",
    "Our code will then look something like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def huber_fn(y_true, y_pred):\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets try out our model on a simple network with a single dense hidden layer. We'll try to fit the cancer dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "cancer = load_breast_cancer()\n",
    "m,n = cancer.data.shape\n",
    "\n",
    "print(cancer.data.shape)\n",
    "\n",
    "X = tf.constant( cancer.data, dtype=tf.float32, name=\"X\" )\n",
    "y = tf.constant( cancer.target.reshape(-1,1), dtype=tf.float32, name=\"y\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Dense(300, input_dim = n))\n",
    "model.add(keras.layers.Dense(2, activation=\"softmax\"))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can view the structure of the network with tensorboard if we include a callback:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "# Windows 10\n",
    "logdir=\"logs\\\\fit\\\\\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "# *nix\n",
    "#logdir=\"logs/fit/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "tensorboard_callback = keras.callbacks.TensorBoard(log_dir=logdir)\n",
    "\n",
    "model.compile(loss=huber_fn, optimizer=\"nadam\", metrics=[\"accuracy\"])\n",
    "model.fit(X, y, epochs=10, callbacks=[tensorboard_callback])\n",
    "\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving Custom Models:\n",
    "\n",
    "Keras will save a model with a custom component (like a custom loss function) as normal, recording all of the custom components we've added. The only trick is that when we reload the model we have to provide Keras with a dictionary mapping the custom objects it expects (in this case a function name \"huber_fn\") to a proper substitute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"my_model_with_a_custom_loss.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = keras.models.load_model(\"my_model_with_a_custom_loss.h5\",\n",
    "                                custom_objects={\"huber_fn\": huber_fn})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if we wanted to extend the Huber loss function to include the $\\delta$ hyperparamter that switches between the linear and quadratic parts? Before digging too deep in it turn out Keras will have a problem: if we set a custom $\\delta$ Keras wont know to save that rate. We could save it in an auxiliary files but that's not scalable. \n",
    "\n",
    "The solution is to define a new __subclass__ of the Keras losses __class__. A __class__ is an object that has some data and some functions associated with it, like a pandas Dataframe or a Numpy array. Both the dataframe and the array aren't just holding data, they have a whole array of functions you can call to manipulate that data. A __class__ is kind of like a machine (like a car):\n",
    "\n",
    "* It has has some internal states. For a machine these would be the amount of fuel, the orientation of the second cog, the number of velocity of a piston in the engine. For us they will be the numbers, arrays, text, etc that we want to store. \n",
    "\n",
    "* It has an interface. For the machine this might be dials to read the internal states and peddles to change them.\n",
    "\n",
    "A subclass is a class that conforms to some expected standards. In this case, Keras has laid out a specific series of commands that it wants a _Loss Function_ class to have. If the class has these functions implemented, Keras will try to use them during training, saving, and other times the loss function will be considered. \n",
    "\n",
    "In the case of the HuberLoss, we need to define three functions:\n",
    "\n",
    "* `__init__()` - Tells Python what to do with the class each time we create a new instance of it.\n",
    "* `call` - Keras expects that the actual computation of the loss function will be performed if it calls `HuberLoss.call`. \n",
    "* `get_config` - When Kera's saves the model, it calls this function to get the configuration of the loss function. \n",
    "\n",
    "The code for the HuberLoss class is below. We'll go through it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HuberLoss(keras.losses.Loss):\n",
    "    def __init__(self, threshold=1.0, **kwargs):\n",
    "#        self.threshold = threshold\n",
    "#        super().__init__(**kwargs)\n",
    "        \n",
    "    def call(self, y_true, y_pred):\n",
    "#        error = y_true - y_pred\n",
    "#        is_small_error = tf.abs(error) < self.threshold\n",
    "#        squared_loss = tf.square(error) / 2\n",
    "#        linear_loss = self.threshold * tf.abs(error) - self.threshold**2 / 2\n",
    "#        return tf.where(is_small_error, squared_loss, linear_loss)\n",
    "    \n",
    "    def get_config(self):\n",
    "#        base_config = super().get_config()\n",
    "#        return {**base_config, \"threshold\": self.threshold}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, when we define the class we use `keras.losses.Loss` to tell python to import all of the machinery the `keras.losses.Loss` class has. This is what is means to __subclass__ a class. On the initialization step, we define a new function\n",
    "\n",
    "    def __init__(self, threshold=1.0, **kwargs):\n",
    "        self.threshold = threshold\n",
    "        super().__init__(**kwargs)\n",
    "        \n",
    "Here, `self` is an object that references the object being created. The variable `**kwargs` is a generic name for a dictionary of keyword's, you'll see it a lot in subclasses as a catchall for any variables that need to be passed to the main class. For loss functions typically need to make a `sum_over_batch_size` call to deal with batch sizes larger than 1, we're going to let the class `keras.losses.Loss` deal with those calls since it know how to. Finally, `threshold=1.0` of course will be our threshold. \n",
    "\n",
    "In the next step, we define a new variable `self.threshold`. Each time we create a new instance of the loss function we set its threshold and can access it using `HuberLoss.theshold`. Finally, we call the initialization function of the `keras.losses.Loss` class, and pass any extra parameters (like `sum_over_batch_size`) up to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = HuberLoss(threshold=1.0)\n",
    "l.threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The call function is the most straight forward: it's just a direct extension of the `huber_fn` function to include the threshold $\\delta$. Note, we always need to pass self to any internal functions so that they can access the classes variables:\n",
    "\n",
    "    def call(self, y_true, y_pred):\n",
    "        error = y_true - y_pred\n",
    "        is_small_error = tf.abs(error) < self.threshold\n",
    "        squared_loss = tf.square(error) / 2\n",
    "        linear_loss = self.threshold * tf.abs(error) - self.threshold**2 / 2\n",
    "        return tf.where(is_small_error, squared_loss, linear_loss)\n",
    "        \n",
    "Finally, the `get_config` function gets the configuration from the larger `keras.losses.Loss` class, and then appends the `self.threshold` parameter with the dictionary key `\"threshold\"` to configuration. It is this configuration that will be saved when Keras saves out the model\n",
    "\n",
    "    \n",
    "    def get_config(self):\n",
    "        base_config = super().get_config()\n",
    "        return {**base_config, \"threshold\": self.threshold}\n",
    "        \n",
    "There's a lot here, an object oriented programing is a large subject, but in general if you want to correctly implement something in Keras, tensorflow, or any other library you should do it by subclassing a standard object instead of creating your own objects. The class information for Keras loss functions can be found here:\n",
    "\n",
    "https://www.tensorflow.org/api_docs/python/tf/keras/losses/Loss\n",
    "\n",
    "If you look at the right hand side, you can see the methods included in the class. \n",
    "\n",
    "Finally, using the class is straight forward:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model.fit(X, y, epochs=1, callbacks=[tensorboard_callback])\n",
    "\n",
    "model.save(\"my_model_with_a_custom_loss.h5\")\n",
    "model2 = keras.models.load_model(\"my_model_with_a_custom_loss.h5\", \n",
    "                                 custom_objects={\"HuberLoss\": HuberLoss},\n",
    "                                 compile=False)\n",
    "#### NOTE: This is currently broken. Can recover using compile=False and recompling the model.\n",
    "\n",
    "# model2.compile(loss=HuberLoss(2.), optimizer=\"nadam\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Activation, Regularization, and Constraints\n",
    "\n",
    "Defining custom functions and classes for activation functions, regularization and constraints is just as easy. For example,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_softplus(z): # return value is just tf.nn.softplus(z)\n",
    "#    return tf.math.log(tf.math.exp(z) + 1.0)\n",
    "\n",
    "def my_glorot_initializer(shape, dtype=tf.float32):\n",
    "#    stddev = tf.sqrt(2. / (shape[0] + shape[1]))\n",
    "#    return tf.random.normal(shape, stddev=stddev, dtype=dtype)\n",
    "\n",
    "def my_l1_regularizer(weights):\n",
    "#    return tf.reduce_sum(tf.abs(0.1 * weights))\n",
    "\n",
    "def my_positive_weights(weights): # return value is just tf.nn.relu(weights)\n",
    "#    return tf.where(weights < 0., tf.zeros_like(weights), weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then can create custom layers using these functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(30, activation=\"elu\", kernel_initializer=\"lecun_normal\",\n",
    "                       input_dim=n),\n",
    "#    keras.layers.Dense(2, activation=my_softplus,\n",
    "#                       kernel_regularizer=my_l1_regularizer,\n",
    "#                       kernel_constraint=my_positive_weights,\n",
    "#                       kernel_initializer=my_glorot_initializer),\n",
    "])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"nadam\", metrics=[\"mae\"])\n",
    "model.fit(X, y, epochs=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course if you want your model to save any hyperparameters these new functions depend on you will need to properly define them as their appropriate subclasses. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Layer and Models\n",
    "\n",
    "If you want to delve into RNN's, or into training deep custom models, there will come a point where you need to start constructing your own custom layers. The simplest kinds of layers are layers with no trainable parameters, also known as functions. If you just want a layer to apply a function the simplest option is to write the function and then wrap it in a lambda layer. For example\n",
    "\n",
    "    exponential_layer = keras.layers.Lambda(lambda x: tf.exp(x))\n",
    "    \n",
    "If you want to build a more complicated custom layer, you should subclass the `keras.layers.Layer` class. A custom layer needs the following methods:\n",
    "\n",
    "* `__init__` - The layer initializer, to be run when a new layer is created. \n",
    "* `build` - The code to initialize the weights and construct the new lay. \n",
    "* `call` - The code to run the layer on input. \n",
    "* `compute_output_shape` - This code will be called by any subsequent layers to determine what their input shape will be. \n",
    "* `get_config` - Returns the layers internal parameters. Called when Keras saves the layer. \n",
    "\n",
    "For example, we'll reinvent the wheel a bit and build a custom dense layer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDense(keras.layers.Layer):\n",
    "    def __init__(self, units, activation=None, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        \n",
    "    def build(self, batch_input_shape):\n",
    "\n",
    "        super().build(batch_input_shape) # must be at the end\n",
    "\n",
    "    def call(self, X):\n",
    "\n",
    "    \n",
    "    def compute_output_shape(self, batch_input_shape):\n",
    "\n",
    "\n",
    "    def get_config(self):\n",
    "        base_config = super().get_config()\n",
    "        return {**base_config, \"units\": self.units,\"activation\": keras.activations.serialize(self.activation)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDense(keras.layers.Layer):\n",
    "    def __init__(self, units, activation=None, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.units = units\n",
    "        self.activation = keras.activations.get(activation)\n",
    "        \n",
    "    def build(self, batch_input_shape):\n",
    "        self.kernel = self.add_weight(\n",
    "            name=\"kernel\", shape=[batch_input_shape[-1], self.units],\n",
    "            initializer=\"glorot_normal\")\n",
    "        self.bias = self.add_weight(\n",
    "            name=\"bias\", shape=[self.units], initializer=\"zeros\")\n",
    "        super().build(batch_input_shape) # must be at the end\n",
    "\n",
    "    def call(self, X):\n",
    "        return self.activation(X @ self.kernel + self.bias)\n",
    "    \n",
    "    def compute_output_shape(self, batch_input_shape):\n",
    "        return tf.TensorShape(batch_input_shape.as_list()[:-1] + [self.units])\n",
    "\n",
    "    def get_config(self):\n",
    "        base_config = super().get_config()\n",
    "        return {**base_config, \"units\": self.units,\"activation\": keras.activations.serialize(self.activation)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### __init__\n",
    "\n",
    "We need to passes the standard initialization parameters to the `keras.layers.Layer` class. Then, we save the number of layer nodes in the internal variable `units`. Finally, if an activation function was passed we save it in the internal variable `activation`. \n",
    "\n",
    "#### build\n",
    "\n",
    "Here, we use the `add_weight` function that comes from the `keras.layers.Layer` to add an array of weights of shape `batch_input_shape` by `self.units` to the later. In this case, `batch_input_shape` will be determined by automatically from the previous layers `compute_output_shape` function. In fact, that function will pass a list of all of the previous layer shapes, so we only take the last one. This is one of the advantages of subclassing, the `keras.layers.Layer` already has functions built in to figure out the number of inputs from the previous layer.  We initialize these weights using Gloro initialization and save the matrix of weights as `self.kernel`. We then add more weights for the bias. \n",
    "\n",
    "\n",
    "#### call\n",
    "\n",
    "Remember that a dense layer is just a matrix multiplication:\n",
    "\n",
    "$$\n",
    "N^{i+1} = \\sigma(N^{i}W + b)\\,.\n",
    "$$\n",
    "\n",
    "In the call method, we compute this function. Remember that for `tf.Tensor` objects, `@` is matrix multiplication. \n",
    "\n",
    "#### compute_output_shape\n",
    "\n",
    "Return the list of input shapes, with the output of this layer appended. This will be called when the next layer is initialized. \n",
    "\n",
    "#### get_config\n",
    "\n",
    "Return the number of units and the activation function used. \n",
    "\n",
    "\n",
    "\n",
    "We can now use `MyDense` as a standard dense layer! Although we didn't implement the `input_dim` for simplicity. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Dense(300, input_dim = n))\n",
    "model.add(MyDense(2, activation=\"softmax\"))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"nadam\", metrics=[\"mae\"])\n",
    "model.fit(X, y, epochs=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise:\n",
    "\n",
    "Implement the `input_dim` parameter. You will need to add a variable to be passed to the `__init__` method that looks like `input_dim=None` so that by default it is set to none. Then in the `__init__` method, same `input_dim` to a new variable called `self.input_dim`.  Finally, in the `build` method use `tf.which` to use the `self.input_dim` is it is not None, and `batch_input_shape` otherwise. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise:\n",
    "\n",
    "The code below constructs a noising layer that adds Guassian noise to input. Use this code, and the code for `MyDense` above to construct a fuzzy dense layer that adds noise to the input and then computes \n",
    "\n",
    "$$\n",
    "N^{i+1} = \\sigma(N^{i}W + b)\\,.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Subclassing Models\n",
    "\n",
    "Note: Part of the explanation for this code is in Chapter 10 under \"Using the Subclassing API to Build Dynamic Models.\"\n",
    "\n",
    "\n",
    "Assume that you want to try 10 different DNN's with varying parameters on a specific dataset. You could code them out each by hand but if they're structurally similar enough its better to create a custom model subclass that can instantiate multiple models with your desired parameters. \n",
    "\n",
    "For example, the code below will build a 2 layer DNN with our specified activation function and layer node count. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoLayerDNN(keras.Model):\n",
    "    def __init__(self, units=[30,30], activation=\"relu\", **kwargs):\n",
    "        super(TwoLayerDNN, self).__init__(**kwargs) # handles standard args (e.g., name)\n",
    "\n",
    "\n",
    "    def call(self, inputs):\n",
    "\n",
    "        return self.out(hidden2)                    \n",
    "\n",
    "\n",
    "model = TwoLayerDNN(units=[40,50])\n",
    "model.build(input_shape=(None, 20))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoLayerDNN(keras.Model):\n",
    "    def __init__(self, units=[30,30], activation=\"relu\", **kwargs):\n",
    "        super(TwoLayerDNN, self).__init__(**kwargs) # handles standard args (e.g., name)\n",
    "        self.units = units\n",
    "        self.hidden1 = keras.layers.Dense(units[0], activation=activation)\n",
    "        self.hidden2 = keras.layers.Dense(units[1], activation=activation)\n",
    "        self.out = keras.layers.Dense(2)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        hidden1 = self.hidden1(inputs)\n",
    "        hidden2 = self.hidden2(hidden1)\n",
    "        return self.out(hidden2)                    \n",
    "\n",
    "\n",
    "model = TwoLayerDNN(units=[40,50])\n",
    "model.build(input_shape=(None, 20))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets use this code to run over a few possible models with a few possible accitvation functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "cancer = load_breast_cancer()\n",
    "m,n = cancer.data.shape\n",
    "\n",
    "print(cancer.data.shape)\n",
    "\n",
    "X = tf.constant( cancer.data, dtype=tf.float32, name=\"X\" )\n",
    "y = tf.constant( cancer.target.reshape(-1,1), dtype=tf.float32, name=\"y\" )\n",
    "\n",
    "model = TwoLayerDNN(units=[40,50])\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"nadam\", metrics=[\"accuracy\"])\n",
    "history = model.fit(X, y, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "unit1 = [10,20,30,40]\n",
    "unit2 = [10,20,30,40]\n",
    "act = [\"relu\",\"elu\",\"selu\"]\n",
    "\n",
    "results = []\n",
    "\n",
    "for i in range(len(unit1)):\n",
    "    for j in range(len(unit2)):\n",
    "        for k in act:\n",
    " #           model = TwoLayerDNN(units=[i,j],activation=k)\n",
    " #           model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"nadam\", metrics=[\"accuracy\"])\n",
    " #           history = model.fit(X, y, epochs=10,verbose=2)\n",
    " #           d = history.history\n",
    " #           d[\"parameters\"] = [i,j,k]\n",
    " #           results = results + [d]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "plt.figure(figsize = (15,15))\n",
    "\n",
    "endacc = []\n",
    "\n",
    "for mod in results:\n",
    "    par = [str(i) for i in mod[\"parameters\"]]\n",
    "    lab = \" \".join(par)\n",
    "    plt.plot(mod[\"accuracy\"],label=lab)\n",
    "    endacc = endacc + [mod[\"accuracy\"][-1]]\n",
    "    \n",
    "plt.legend()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can find the optimal parameters as well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endacc = [mod[\"accuracy\"][-1] for mod in results ]\n",
    "np.argmax(endacc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results[24][\"parameters\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, we don't need to do an exhaustive search either, there are quite a few tool kits available for hyperparmeter tuning including Hyperopt, Keras Tuner, Scikit-Optimize and GridSearchCV in scikit learn. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra: \n",
    "\n",
    "Lets try to make our chart more useful by using coloring that actually is responsive to the different parameters we're using:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "unit1 = [10,20,30,40]\n",
    "unit2 = [10,20,30,40]\n",
    "act = [\"relu\",\"elu\",\"selu\"]\n",
    "\n",
    "results = []\n",
    "\n",
    "for i in range(len(unit1)):\n",
    "    for j in range(len(unit2)):\n",
    "        for k in range(len(act)):\n",
    "            model = TwoLayerDNN(units=[unit1[i],unit2[j]],activation=act[k])\n",
    "            model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"nadam\", metrics=[\"accuracy\"])\n",
    "            history = model.fit(X, y, epochs=10,verbose=2)\n",
    "            d = history.history\n",
    "            d[\"parameters\"] = [i,j,k]\n",
    "            results = results + [d]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "plt.figure(figsize = (15,15))\n",
    "\n",
    "endacc = []\n",
    "\n",
    "al = [.3,.5,.7,.9]\n",
    "cs = [\"C0\", \"C1\", \"C2\", \"C3\"]\n",
    "st = [':', '-', '--']\n",
    "\n",
    "for mod in results:\n",
    "    p = mod[\"parameters\"]\n",
    "    par = [str(unit1[p[0]]), str(unit2[p[1]]), str(act[p[2]])]\n",
    "    lab = \" \".join(par)\n",
    "    plt.plot(mod[\"accuracy\"],\n",
    "             linestyle=st[p[2]],\n",
    "             color=cs[p[1]],\n",
    "             alpha = al[p[0]],\n",
    "             label=lab)\n",
    "    endacc = endacc + [mod[\"accuracy\"][-1]]\n",
    "    \n",
    "plt.legend()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise:\n",
    "\n",
    "Create a subclass that also allow you to select the number of layers. Since you need to to have the layers saved in a fixed variable, instead of using \n",
    "\n",
    "`self.hidden1 = keras.layers.Dense(units[0], activation=activation)`\n",
    "\n",
    "in in the initialization create a `list` object containing all of your layers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
