{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# Two Problems for Lab 3:\n",
    "\n",
    "\n",
    "# Problem 1: Gender Recognition by Voice\n",
    "\n",
    "From the description file at https://data.world/ml-research/gender-recognition-by-voice:\n",
    "\n",
    "In order to analyze gender by voice and speech, a training database was required. A database was built using thousands of samples of male and female voices, each labeled by their gender of male or female. Voice samples were collected from the following resources:\n",
    "\n",
    "*  [The Harvard-Haskins Database of Regularly-Timed Speech](http://nsi.wegall.net/)\n",
    "*  Telecommunications & Signal Processing Laboratory (TSP) Speech Database at McGill University\n",
    "*  [VoxForge Speech Corpus](http://www.repository.voxforge1.org/downloads/SpeechCorpus/Trunk/Audio/Main/8kHz_16bit/)\n",
    "*  [Festvox CMU_ARCTIC Speech Database at Carnegie Mellon University](http://festvox.org/cmu_arctic/dbs_awb.html)\n",
    "\n",
    "Each voice sample is stored as a .WAV file, which is then pre-processed for acoustic analysis using the specan function from the WarbleR R package. Specan measures 22 acoustic parameters on acoustic signals for which the start and end times are provided.\n",
    "\n",
    "The output from the pre-processed WAV files were saved into a CSV file, containing 3168 rows and 21 columns (20 columns for each feature and one label column for the classification of male or female). You can download the pre-processed dataset in CSV format, using the link above\n",
    "Acoustic Properties Measured\n",
    "\n",
    "The following acoustic properties of each voice are measured:\n",
    "\n",
    "*    __duration:__ length of signal\n",
    "*    __meanfreq:__ mean frequency (in kHz)\n",
    "*    __sd:__ standard deviation of frequency\n",
    "*    __median:__ median frequency (in kHz)\n",
    "*    __Q25:__ first quantile (in kHz)\n",
    "*    __Q75:__ third quantile (in kHz)\n",
    "*    __IQR:__ interquantile range (in kHz)\n",
    "*    __skew:__ skewness (see note in specprop description)\n",
    "*    __kurt:__ kurtosis (see note in specprop description)\n",
    "*    __sp.ent:__ spectral entropy\n",
    "*    __sfm:__ spectral flatness\n",
    "*    __mode:__ mode frequency\n",
    "*    __centroid:__ frequency centroid (see specprop)\n",
    "*    __peakf:__ peak frequency (frequency with highest energy)\n",
    "*    __meanfun:__ average of fundamental frequency measured across acoustic signal\n",
    "*    __minfun:__ minimum fundamental frequency measured across acoustic signal\n",
    "*    __maxfun:__ maximum fundamental frequency measured across acoustic signal\n",
    "*    __meandom:__ average of dominant frequency measured across acoustic signal\n",
    "*    __mindom:__ minimum of dominant frequency measured across acoustic signal\n",
    "*    __maxdom:__ maximum of dominant frequency measured across acoustic signal\n",
    "*    __dfrange:__ range of dominant frequency measured across acoustic signal\n",
    "*    __modindx:__ modulation index. Calculated as the accumulated absolute difference between adjacent measurements of fundamental frequencies divided by the frequency range\n",
    "\n",
    "The gender of the speaker is given in the __label__ column. \n",
    "\n",
    "Note, the features for duration and peak frequency (peakf) were removed from training. Duration refers to the length of the recording, which for training, is cut off at 20 seconds. Peakf was omitted from calculation due to time and CPU constraints in calculating the value. In this case, all records will have the same value for duration (20) and peak frequency (0).\n",
    "\n",
    "Load file using the code below. \n",
    "\n",
    "#### Question 1:\n",
    "\n",
    "Which two features are most indicative of gendered voice?\n",
    "\n",
    "#### Question 2:\n",
    "\n",
    "Preform Linear Regression, Logistic Regression, and Quadratic Discriminant Analysis on the features, graphing the resulting fits. How does the two feature fit compare to the fit on all features?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>meanfreq</th>\n",
       "      <th>sd</th>\n",
       "      <th>median</th>\n",
       "      <th>Q25</th>\n",
       "      <th>Q75</th>\n",
       "      <th>IQR</th>\n",
       "      <th>skew</th>\n",
       "      <th>kurt</th>\n",
       "      <th>sp.ent</th>\n",
       "      <th>sfm</th>\n",
       "      <th>...</th>\n",
       "      <th>centroid</th>\n",
       "      <th>meanfun</th>\n",
       "      <th>minfun</th>\n",
       "      <th>maxfun</th>\n",
       "      <th>meandom</th>\n",
       "      <th>mindom</th>\n",
       "      <th>maxdom</th>\n",
       "      <th>dfrange</th>\n",
       "      <th>modindx</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.059781</td>\n",
       "      <td>0.064241</td>\n",
       "      <td>0.032027</td>\n",
       "      <td>0.015071</td>\n",
       "      <td>0.090193</td>\n",
       "      <td>0.075122</td>\n",
       "      <td>12.863462</td>\n",
       "      <td>274.402906</td>\n",
       "      <td>0.893369</td>\n",
       "      <td>0.491918</td>\n",
       "      <td>...</td>\n",
       "      <td>0.059781</td>\n",
       "      <td>0.084279</td>\n",
       "      <td>0.015702</td>\n",
       "      <td>0.275862</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.066009</td>\n",
       "      <td>0.067310</td>\n",
       "      <td>0.040229</td>\n",
       "      <td>0.019414</td>\n",
       "      <td>0.092666</td>\n",
       "      <td>0.073252</td>\n",
       "      <td>22.423285</td>\n",
       "      <td>634.613855</td>\n",
       "      <td>0.892193</td>\n",
       "      <td>0.513724</td>\n",
       "      <td>...</td>\n",
       "      <td>0.066009</td>\n",
       "      <td>0.107937</td>\n",
       "      <td>0.015826</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.009014</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.054688</td>\n",
       "      <td>0.046875</td>\n",
       "      <td>0.052632</td>\n",
       "      <td>male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.077316</td>\n",
       "      <td>0.083829</td>\n",
       "      <td>0.036718</td>\n",
       "      <td>0.008701</td>\n",
       "      <td>0.131908</td>\n",
       "      <td>0.123207</td>\n",
       "      <td>30.757155</td>\n",
       "      <td>1024.927705</td>\n",
       "      <td>0.846389</td>\n",
       "      <td>0.478905</td>\n",
       "      <td>...</td>\n",
       "      <td>0.077316</td>\n",
       "      <td>0.098706</td>\n",
       "      <td>0.015656</td>\n",
       "      <td>0.271186</td>\n",
       "      <td>0.007990</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.046512</td>\n",
       "      <td>male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.151228</td>\n",
       "      <td>0.072111</td>\n",
       "      <td>0.158011</td>\n",
       "      <td>0.096582</td>\n",
       "      <td>0.207955</td>\n",
       "      <td>0.111374</td>\n",
       "      <td>1.232831</td>\n",
       "      <td>4.177296</td>\n",
       "      <td>0.963322</td>\n",
       "      <td>0.727232</td>\n",
       "      <td>...</td>\n",
       "      <td>0.151228</td>\n",
       "      <td>0.088965</td>\n",
       "      <td>0.017798</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.201497</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.562500</td>\n",
       "      <td>0.554688</td>\n",
       "      <td>0.247119</td>\n",
       "      <td>male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.135120</td>\n",
       "      <td>0.079146</td>\n",
       "      <td>0.124656</td>\n",
       "      <td>0.078720</td>\n",
       "      <td>0.206045</td>\n",
       "      <td>0.127325</td>\n",
       "      <td>1.101174</td>\n",
       "      <td>4.333713</td>\n",
       "      <td>0.971955</td>\n",
       "      <td>0.783568</td>\n",
       "      <td>...</td>\n",
       "      <td>0.135120</td>\n",
       "      <td>0.106398</td>\n",
       "      <td>0.016931</td>\n",
       "      <td>0.266667</td>\n",
       "      <td>0.712812</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>5.484375</td>\n",
       "      <td>5.476562</td>\n",
       "      <td>0.208274</td>\n",
       "      <td>male</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   meanfreq        sd    median       Q25       Q75       IQR       skew  \\\n",
       "0  0.059781  0.064241  0.032027  0.015071  0.090193  0.075122  12.863462   \n",
       "1  0.066009  0.067310  0.040229  0.019414  0.092666  0.073252  22.423285   \n",
       "2  0.077316  0.083829  0.036718  0.008701  0.131908  0.123207  30.757155   \n",
       "3  0.151228  0.072111  0.158011  0.096582  0.207955  0.111374   1.232831   \n",
       "4  0.135120  0.079146  0.124656  0.078720  0.206045  0.127325   1.101174   \n",
       "\n",
       "          kurt    sp.ent       sfm  ...  centroid   meanfun    minfun  \\\n",
       "0   274.402906  0.893369  0.491918  ...  0.059781  0.084279  0.015702   \n",
       "1   634.613855  0.892193  0.513724  ...  0.066009  0.107937  0.015826   \n",
       "2  1024.927705  0.846389  0.478905  ...  0.077316  0.098706  0.015656   \n",
       "3     4.177296  0.963322  0.727232  ...  0.151228  0.088965  0.017798   \n",
       "4     4.333713  0.971955  0.783568  ...  0.135120  0.106398  0.016931   \n",
       "\n",
       "     maxfun   meandom    mindom    maxdom   dfrange   modindx  label  \n",
       "0  0.275862  0.007812  0.007812  0.007812  0.000000  0.000000   male  \n",
       "1  0.250000  0.009014  0.007812  0.054688  0.046875  0.052632   male  \n",
       "2  0.271186  0.007990  0.007812  0.015625  0.007812  0.046512   male  \n",
       "3  0.250000  0.201497  0.007812  0.562500  0.554688  0.247119   male  \n",
       "4  0.266667  0.712812  0.007812  5.484375  5.476562  0.208274   male  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv(\"voice.csv\")\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 2: MRI Data\n",
    "\n",
    "The dementia level for the Oasis 1 MRI dataset is based on a patient assessment. As a result, it is not clear whether the levels of 0, .5, 1 and 2 should actually be understood as meaningfully numeric, or if they in fact are categorical labels. \n",
    "\n",
    "In this problem we want to treat them as categorical. However, we would also like to construct a slightly larger dataset, as we have seen that for images our 700 may not be sufficient. To construct a larger dataset we will again down sample the images, however this time we will use the down sampling to expand the dataset instead of throwing data away. After fixing a down sample rate $D$, we will construct one image out of the pixels $nD$, for $n = 1,2,\\ldots, $. We will also construct $n D+i$, for $i = 1,\\ldots, D$. This way, by down sampling with a rate $D$, we construct $D$ more pictures. \n",
    "\n",
    "__Note:__ It is very import that we perform the train test split _before_ we expand the dataset through down sampling. If not, we are effectively training on the test data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Filename</th>\n",
       "      <th>ID</th>\n",
       "      <th>M/F</th>\n",
       "      <th>Hand</th>\n",
       "      <th>Age</th>\n",
       "      <th>Educ</th>\n",
       "      <th>SES</th>\n",
       "      <th>MMSE</th>\n",
       "      <th>CDR</th>\n",
       "      <th>eTIV</th>\n",
       "      <th>nWBV</th>\n",
       "      <th>ASF</th>\n",
       "      <th>Delay</th>\n",
       "      <th>Slice</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>OAS1_0001_MR1_55.png</td>\n",
       "      <td>OAS1_0001_MR1</td>\n",
       "      <td>F</td>\n",
       "      <td>R</td>\n",
       "      <td>74</td>\n",
       "      <td>2</td>\n",
       "      <td>3.0</td>\n",
       "      <td>29</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1344</td>\n",
       "      <td>0.743</td>\n",
       "      <td>1.306</td>\n",
       "      <td>NaN</td>\n",
       "      <td>55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>OAS1_0001_MR1_120.png</td>\n",
       "      <td>OAS1_0001_MR1</td>\n",
       "      <td>F</td>\n",
       "      <td>R</td>\n",
       "      <td>74</td>\n",
       "      <td>2</td>\n",
       "      <td>3.0</td>\n",
       "      <td>29</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1344</td>\n",
       "      <td>0.743</td>\n",
       "      <td>1.306</td>\n",
       "      <td>NaN</td>\n",
       "      <td>120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>OAS1_0001_MR1_180.png</td>\n",
       "      <td>OAS1_0001_MR1</td>\n",
       "      <td>F</td>\n",
       "      <td>R</td>\n",
       "      <td>74</td>\n",
       "      <td>2</td>\n",
       "      <td>3.0</td>\n",
       "      <td>29</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1344</td>\n",
       "      <td>0.743</td>\n",
       "      <td>1.306</td>\n",
       "      <td>NaN</td>\n",
       "      <td>180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>OAS1_0002_MR1_55.png</td>\n",
       "      <td>OAS1_0002_MR1</td>\n",
       "      <td>F</td>\n",
       "      <td>R</td>\n",
       "      <td>55</td>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>29</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1147</td>\n",
       "      <td>0.810</td>\n",
       "      <td>1.531</td>\n",
       "      <td>NaN</td>\n",
       "      <td>55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>OAS1_0002_MR1_120.png</td>\n",
       "      <td>OAS1_0002_MR1</td>\n",
       "      <td>F</td>\n",
       "      <td>R</td>\n",
       "      <td>55</td>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>29</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1147</td>\n",
       "      <td>0.810</td>\n",
       "      <td>1.531</td>\n",
       "      <td>NaN</td>\n",
       "      <td>120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>604</th>\n",
       "      <td>604</td>\n",
       "      <td>OAS1_0449_MR1_120.png</td>\n",
       "      <td>OAS1_0449_MR1</td>\n",
       "      <td>F</td>\n",
       "      <td>R</td>\n",
       "      <td>71</td>\n",
       "      <td>3</td>\n",
       "      <td>4.0</td>\n",
       "      <td>29</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1264</td>\n",
       "      <td>0.818</td>\n",
       "      <td>1.388</td>\n",
       "      <td>NaN</td>\n",
       "      <td>120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>605</th>\n",
       "      <td>605</td>\n",
       "      <td>OAS1_0449_MR1_180.png</td>\n",
       "      <td>OAS1_0449_MR1</td>\n",
       "      <td>F</td>\n",
       "      <td>R</td>\n",
       "      <td>71</td>\n",
       "      <td>3</td>\n",
       "      <td>4.0</td>\n",
       "      <td>29</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1264</td>\n",
       "      <td>0.818</td>\n",
       "      <td>1.388</td>\n",
       "      <td>NaN</td>\n",
       "      <td>180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>606</th>\n",
       "      <td>606</td>\n",
       "      <td>OAS1_0456_MR1_55.png</td>\n",
       "      <td>OAS1_0456_MR1</td>\n",
       "      <td>M</td>\n",
       "      <td>R</td>\n",
       "      <td>61</td>\n",
       "      <td>5</td>\n",
       "      <td>2.0</td>\n",
       "      <td>30</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1637</td>\n",
       "      <td>0.780</td>\n",
       "      <td>1.072</td>\n",
       "      <td>NaN</td>\n",
       "      <td>55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>607</th>\n",
       "      <td>607</td>\n",
       "      <td>OAS1_0456_MR1_120.png</td>\n",
       "      <td>OAS1_0456_MR1</td>\n",
       "      <td>M</td>\n",
       "      <td>R</td>\n",
       "      <td>61</td>\n",
       "      <td>5</td>\n",
       "      <td>2.0</td>\n",
       "      <td>30</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1637</td>\n",
       "      <td>0.780</td>\n",
       "      <td>1.072</td>\n",
       "      <td>NaN</td>\n",
       "      <td>120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>608</th>\n",
       "      <td>608</td>\n",
       "      <td>OAS1_0456_MR1_180.png</td>\n",
       "      <td>OAS1_0456_MR1</td>\n",
       "      <td>M</td>\n",
       "      <td>R</td>\n",
       "      <td>61</td>\n",
       "      <td>5</td>\n",
       "      <td>2.0</td>\n",
       "      <td>30</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1637</td>\n",
       "      <td>0.780</td>\n",
       "      <td>1.072</td>\n",
       "      <td>NaN</td>\n",
       "      <td>180</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>609 rows × 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Unnamed: 0               Filename             ID M/F Hand  Age  Educ  \\\n",
       "0             0   OAS1_0001_MR1_55.png  OAS1_0001_MR1   F    R   74     2   \n",
       "1             1  OAS1_0001_MR1_120.png  OAS1_0001_MR1   F    R   74     2   \n",
       "2             2  OAS1_0001_MR1_180.png  OAS1_0001_MR1   F    R   74     2   \n",
       "3             3   OAS1_0002_MR1_55.png  OAS1_0002_MR1   F    R   55     4   \n",
       "4             4  OAS1_0002_MR1_120.png  OAS1_0002_MR1   F    R   55     4   \n",
       "..          ...                    ...            ...  ..  ...  ...   ...   \n",
       "604         604  OAS1_0449_MR1_120.png  OAS1_0449_MR1   F    R   71     3   \n",
       "605         605  OAS1_0449_MR1_180.png  OAS1_0449_MR1   F    R   71     3   \n",
       "606         606   OAS1_0456_MR1_55.png  OAS1_0456_MR1   M    R   61     5   \n",
       "607         607  OAS1_0456_MR1_120.png  OAS1_0456_MR1   M    R   61     5   \n",
       "608         608  OAS1_0456_MR1_180.png  OAS1_0456_MR1   M    R   61     5   \n",
       "\n",
       "     SES  MMSE  CDR  eTIV   nWBV    ASF  Delay  Slice  \n",
       "0    3.0    29  0.0  1344  0.743  1.306    NaN     55  \n",
       "1    3.0    29  0.0  1344  0.743  1.306    NaN    120  \n",
       "2    3.0    29  0.0  1344  0.743  1.306    NaN    180  \n",
       "3    1.0    29  0.0  1147  0.810  1.531    NaN     55  \n",
       "4    1.0    29  0.0  1147  0.810  1.531    NaN    120  \n",
       "..   ...   ...  ...   ...    ...    ...    ...    ...  \n",
       "604  4.0    29  0.0  1264  0.818  1.388    NaN    120  \n",
       "605  4.0    29  0.0  1264  0.818  1.388    NaN    180  \n",
       "606  2.0    30  0.0  1637  0.780  1.072    NaN     55  \n",
       "607  2.0    30  0.0  1637  0.780  1.072    NaN    120  \n",
       "608  2.0    30  0.0  1637  0.780  1.072    NaN    180  \n",
       "\n",
       "[609 rows x 15 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib\n",
    "\n",
    "file_dir = '/Users/wang/Dropbox/z7243MachineLearning/Labs/Lab 2/MRI_Images/'\n",
    "\n",
    "labels = pd.read_csv(file_dir + 'labels.csv')\n",
    "display(labels)\n",
    "y = labels.CDR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(487,) (122,)\n"
     ]
    }
   ],
   "source": [
    "data = np.zeros([609, 30976])\n",
    "\n",
    "for n, file_name in enumerate(labels.Filename):\n",
    "    data[n,:] = np.mean(matplotlib.image.imread(file_dir + file_name),axis=2).reshape(-1)\n",
    "\n",
    "    \n",
    "    \n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, y, test_size=0.2, random_state=0)\n",
    "print(y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to sample the data array using the `data[start:stop:step]` slice paradigm. This means we are taking elements of the array `data` starting at `start`, ending at `stop` with step `step`. This is why previously `data[::DS]` down sampled at a rate of DS. For example, \n",
    "\n",
    "    lst = list(range(165)); lst[6::10]\n",
    "    \n",
    "returns\n",
    "\n",
    "    [6, 16, 26, 36, 46, 56, 66, 76, 86, 96, 106, 116, 126, 136, 146, 156]\n",
    "\n",
    "We need to create two new arrays, one of shape $[561\\times DS, 30976/DS]$ containing the down sampled data, and one of shape $[561\\times DS]$ containing the labels. The for each of the $N_{train}$ images in the training array, we need to create $DS$ new down sampled images, with the downsample starting from $i$:\n",
    "\n",
    "`Xds_train[n+i, :] = X_train[i::DS]`\n",
    "\n",
    "This will split our images into DS down sampled images. We then need to be sure to save out the appropriate label:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3896, 3872)\n"
     ]
    }
   ],
   "source": [
    "DS = 8             # Downsample rate, must be a multiple of 30976\n",
    "\n",
    "N_train = y_train.shape[0]  # The length of the training data\n",
    "\n",
    "if 30976/DS % 1 > 0:\n",
    "    print(\"Downsample rate is not a multiple of 30976\")\n",
    "    DS = 1\n",
    "    im_size = 30976\n",
    "else:\n",
    "    im_size = int(30976/DS)\n",
    "\n",
    "Xds_train = np.zeros([N_train*DS, im_size])\n",
    "yds_train = np.zeros([N_train*DS, im_size])\n",
    "    \n",
    "for n in range(N_train):\n",
    "    for i in range(DS):\n",
    "        Xds_train[n+i,:] = X_train[n,i::DS]\n",
    "        yds_train[n+i] = y[n]\n",
    "        \n",
    "print(Xds_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3896"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N_train*DS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "487"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1:\n",
    "\n",
    "Based on the code above, downsample the test data in the same way.  ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2:\n",
    "\n",
    "Perform LDA, QDA, Logistic Regression and Categorical Linear Regression on the down sampled Oasis 1 dataset. How do these compare to linear regression?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
